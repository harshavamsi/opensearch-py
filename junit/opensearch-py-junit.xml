<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="35" failures="0" skipped="47" tests="293" time="37.546" timestamp="2022-09-29T18:32:04.470077" hostname="dev-dsk-hvamsi-2b-70161453.us-west-2.amazon.com"><testcase classname="test_opensearchpy.test_cases.TestOpenSearchTestCase" name="test_each_call_is_recorded" file="test_opensearchpy/test_cases.py" line="71" time="0.002" /><testcase classname="test_opensearchpy.test_cases.TestOpenSearchTestCase" name="test_our_transport_used" file="test_opensearchpy/test_cases.py" line="65" time="0.001" /><testcase classname="test_opensearchpy.test_cases.TestOpenSearchTestCase" name="test_start_with_0_call" file="test_opensearchpy/test_cases.py" line="68" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestBaseConnection" name="test_compatibility_accept_header" file="test_opensearchpy/test_connection.py" line="136" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestBaseConnection" name="test_empty_warnings" file="test_opensearchpy/test_connection.py" line="70" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestBaseConnection" name="test_ipv6_host_and_port" file="test_opensearchpy/test_connection.py" line="125" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestBaseConnection" name="test_raises_errors" file="test_opensearchpy/test_connection.py" line="112" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestBaseConnection" name="test_raises_warnings" file="test_opensearchpy/test_connection.py" line="78" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestBaseConnection" name="test_raises_warnings_when_folded" file="test_opensearchpy/test_connection.py" line="100" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_aws_signer_as_http_auth" file="test_opensearchpy/test_connection.py" line="288" time="0.026" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_aws_signer_when_credentials_is_null" file="test_opensearchpy/test_connection.py" line="320" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_aws_signer_when_region_is_null" file="test_opensearchpy/test_connection.py" line="307" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_default_user_agent" file="test_opensearchpy/test_connection.py" line="230" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_doesnt_use_https_if_not_specified" file="test_opensearchpy/test_connection.py" line="363" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_http_auth" file="test_opensearchpy/test_connection.py" line="252" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_http_auth_list" file="test_opensearchpy/test_connection.py" line="276" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_http_auth_tuple" file="test_opensearchpy/test_connection.py" line="264" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_http_compression" file="test_opensearchpy/test_connection.py" line="205" time="0.003" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_keep_alive_is_on_by_default" file="test_opensearchpy/test_connection.py" line="241" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_no_http_compression" file="test_opensearchpy/test_connection.py" line="192" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_no_warning_when_using_ssl_context" file="test_opensearchpy/test_connection.py" line="367" time="0.008" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_nowarn_when_uses_https_if_verify_certs_is_off" file="test_opensearchpy/test_connection.py" line="354" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_opaque_id" file="test_opensearchpy/test_connection.py" line="188" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_recursion_error_reraised" file="test_opensearchpy/test_connection.py" line="412" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_ssl_context" file="test_opensearchpy/test_connection.py" line="172" time="0.007" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_surrogatepass_into_bytes" file="test_opensearchpy/test_connection.py" line="406" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_timeout_set" file="test_opensearchpy/test_connection.py" line="237" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_uncompressed_body_logged" file="test_opensearchpy/test_connection.py" line="395" time="0.003" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_uses_https_if_verify_certs_is_off" file="test_opensearchpy/test_connection.py" line="343" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestUrllib3Connection" name="test_warns_if_using_non_default_ssl_kwargs_with_ssl_context" file="test_opensearchpy/test_connection.py" line="373" time="0.039" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_body_attached" file="test_opensearchpy/test_connection.py" line="719" time="0.003" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_conflict_error_is_returned_on_409" file="test_opensearchpy/test_connection.py" line="592" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_custom_headers" file="test_opensearchpy/test_connection.py" line="560" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_custom_http_auth_is_allowed" file="test_opensearchpy/test_connection.py" line="461" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_default_headers" file="test_opensearchpy/test_connection.py" line="554" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_defaults" file="test_opensearchpy/test_connection.py" line="701" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_failed_request_logs_and_traces" file="test_opensearchpy/test_connection.py" line="610" time="0.008" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_head_with_404_doesnt_get_logged" file="test_opensearchpy/test_connection.py" line="604" time="0.003" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_http_auth" file="test_opensearchpy/test_connection.py" line="574" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_http_auth_attached" file="test_opensearchpy/test_connection.py" line="727" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_http_auth_list" file="test_opensearchpy/test_connection.py" line="582" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_http_auth_tuple" file="test_opensearchpy/test_connection.py" line="578" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_http_compression" file="test_opensearchpy/test_connection.py" line="487" time="0.004" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_merge_headers" file="test_opensearchpy/test_connection.py" line="545" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_no_http_compression" file="test_opensearchpy/test_connection.py" line="475" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_not_found_error_is_returned_on_404" file="test_opensearchpy/test_connection.py" line="596" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_nowarn_when_uses_https_if_verify_certs_is_off" file="test_opensearchpy/test_connection.py" line="527" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_opaque_id" file="test_opensearchpy/test_connection.py" line="471" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_params_properly_encoded" file="test_opensearchpy/test_connection.py" line="709" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_recursion_error_reraised" file="test_opensearchpy/test_connection.py" line="757" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_repr" file="test_opensearchpy/test_connection.py" line="586" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_request_error_is_returned_on_400" file="test_opensearchpy/test_connection.py" line="600" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_success_logs_and_traces" file="test_opensearchpy/test_connection.py" line="638" time="0.008" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_surrogatepass_into_bytes" file="test_opensearchpy/test_connection.py" line="751" time="0.002" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_timeout_set" file="test_opensearchpy/test_connection.py" line="467" time="0.001" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_uncompressed_body_logged" file="test_opensearchpy/test_connection.py" line="678" time="0.006" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_url_prefix" file="test_opensearchpy/test_connection.py" line="733" time="0.006" /><testcase classname="test_opensearchpy.test_connection.TestRequestsConnection" name="test_uses_https_if_verify_certs_is_off" file="test_opensearchpy/test_connection.py" line="510" time="0.003" /><testcase classname="test_opensearchpy.test_connection.TestConnectionHttpbin" name="test_urllib3_connection" file="test_opensearchpy/test_connection.py" line="784" time="1.144" /><testcase classname="test_opensearchpy.test_connection.TestConnectionHttpbin" name="test_urllib3_connection_error" file="test_opensearchpy/test_connection.py" line="846" time="0.003" /><testcase classname="test_opensearchpy.test_connection.TestConnectionHttpbin" name="test_requests_connection" file="test_opensearchpy/test_connection.py" line="851" time="1.124" /><testcase classname="test_opensearchpy.test_connection.TestConnectionHttpbin" name="test_requests_connection_error" file="test_opensearchpy/test_connection.py" line="913" time="0.004" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_already_failed_connection_has_longer_timeout" file="test_opensearchpy/test_connection_pool.py" line="139" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_connection_is_forcibly_resurrected_when_no_live_ones_are_availible" file="test_opensearchpy/test_connection_pool.py" line="112" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_connection_is_resurrected_after_its_timeout" file="test_opensearchpy/test_connection_pool.py" line="122" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_connection_is_skipped_when_dead" file="test_opensearchpy/test_connection_pool.py" line="92" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_dead_count_is_wiped_clean_for_connection_if_marked_live" file="test_opensearchpy/test_connection_pool.py" line="157" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_dead_nodes_are_removed_from_active_connections" file="test_opensearchpy/test_connection_pool.py" line="83" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_default_round_robin" file="test_opensearchpy/test_connection_pool.py" line="49" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_disable_shuffling" file="test_opensearchpy/test_connection_pool.py" line="57" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_dummy_cp_raises_exception_on_more_connections" file="test_opensearchpy/test_connection_pool.py" line="40" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_force_resurrect_always_returns_a_connection" file="test_opensearchpy/test_connection_pool.py" line="131" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_new_connection_is_not_marked_dead" file="test_opensearchpy/test_connection_pool.py" line="101" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_raises_exception_when_no_connections_defined" file="test_opensearchpy/test_connection_pool.py" line="46" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_selectors_have_access_to_connection_opts" file="test_opensearchpy/test_connection_pool.py" line="65" time="0.001" /><testcase classname="test_opensearchpy.test_connection_pool.TestConnectionPool" name="test_timeout_for_failed_connections_is_limitted" file="test_opensearchpy/test_connection_pool.py" line="148" time="0.001" /><testcase classname="test_opensearchpy.test_exceptions.TestTransformError" name="test_transform_error_parse_with_error_reason" file="test_opensearchpy/test_exceptions.py" line="32" time="0.001" /><testcase classname="test_opensearchpy.test_exceptions.TestTransformError" name="test_transform_error_parse_with_error_string" file="test_opensearchpy/test_exceptions.py" line="43" time="0.001" /><testcase classname="test_opensearchpy.test_helpers.TestParallelBulk" name="test_all_chunks_sent" file="test_opensearchpy/test_helpers.py" line="57" time="1.313" /><testcase classname="test_opensearchpy.test_helpers.TestParallelBulk" name="test_chunk_sent_from_different_threads" file="test_opensearchpy/test_helpers.py" line="67" time="0.001"><skipped type="pytest.skip" message="unconditional skip">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_helpers.py:68: unconditional skip</skipped></testcase><testcase classname="test_opensearchpy.test_helpers.TestChunkActions" name="test__source_metadata_or_source" file="test_opensearchpy/test_helpers.py" line="155" time="0.001" /><testcase classname="test_opensearchpy.test_helpers.TestChunkActions" name="test_chunks_are_chopped_by_byte_size" file="test_opensearchpy/test_helpers.py" line="183" time="0.003" /><testcase classname="test_opensearchpy.test_helpers.TestChunkActions" name="test_chunks_are_chopped_by_byte_size_properly" file="test_opensearchpy/test_helpers.py" line="201" time="0.003" /><testcase classname="test_opensearchpy.test_helpers.TestChunkActions" name="test_chunks_are_chopped_by_chunk_size" file="test_opensearchpy/test_helpers.py" line="191" time="0.003" /><testcase classname="test_opensearchpy.test_helpers.TestChunkActions" name="test_expand_action" file="test_opensearchpy/test_helpers.py" line="87" time="0.001" /><testcase classname="test_opensearchpy.test_helpers.TestChunkActions" name="test_expand_action_actions" file="test_opensearchpy/test_helpers.py" line="93" time="0.001" /><testcase classname="test_opensearchpy.test_helpers.TestChunkActions" name="test_expand_action_options" file="test_opensearchpy/test_helpers.py" line="124" time="0.002" /><testcase classname="test_opensearchpy.test_helpers.TestExpandActions" name="test_string_actions_are_marked_as_simple_inserts" file="test_opensearchpy/test_helpers.py" line="216" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_datetime_serialization" file="test_opensearchpy/test_serializer.py" line="55" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_decimal_serialization" file="test_opensearchpy/test_serializer.py" line="61" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:62: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_raises_serialization_error_on_dump_error" file="test_opensearchpy/test_serializer.py" line="192" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_raises_serialization_error_on_load_error" file="test_opensearchpy/test_serializer.py" line="195" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_raises_serialization_error_pandas_nat" file="test_opensearchpy/test_serializer.py" line="170" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:171: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_numpy_bool" file="test_opensearchpy/test_serializer.py" line="76" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:77: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_numpy_datetime" file="test_opensearchpy/test_serializer.py" line="115" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:116: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_numpy_floats" file="test_opensearchpy/test_serializer.py" line="102" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:103: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_numpy_integers" file="test_opensearchpy/test_serializer.py" line="81" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:82: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_numpy_nan_to_nan" file="test_opensearchpy/test_serializer.py" line="136" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:137: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_numpy_ndarray" file="test_opensearchpy/test_serializer.py" line="123" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:124: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_pandas_category" file="test_opensearchpy/test_serializer.py" line="177" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:178: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_pandas_na" file="test_opensearchpy/test_serializer.py" line="160" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:161: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_pandas_series" file="test_opensearchpy/test_serializer.py" line="152" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:153: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_serializes_pandas_timestamp" file="test_opensearchpy/test_serializer.py" line="144" time="0.001"><skipped type="pytest.skip" message="Test requires numpy or pandas to be available">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_serializer.py:145: Test requires numpy or pandas to be available</skipped></testcase><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_strings_are_left_untouched" file="test_opensearchpy/test_serializer.py" line="200" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestJSONSerializer" name="test_uuid_serialization" file="test_opensearchpy/test_serializer.py" line="68" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestTextSerializer" name="test_raises_serialization_error_on_dump_error" file="test_opensearchpy/test_serializer.py" line="208" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestTextSerializer" name="test_strings_are_left_untouched" file="test_opensearchpy/test_serializer.py" line="205" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestDeserializer" name="test_deserializes_json_by_default" file="test_opensearchpy/test_serializer.py" line="216" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestDeserializer" name="test_deserializes_text_with_correct_ct" file="test_opensearchpy/test_serializer.py" line="219" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestDeserializer" name="test_raises_improperly_configured_when_default_mimetype_cannot_be_deserialized" file="test_opensearchpy/test_serializer.py" line="231" time="0.001" /><testcase classname="test_opensearchpy.test_serializer.TestDeserializer" name="test_raises_serialization_error_on_unknown_mimetype" file="test_opensearchpy/test_serializer.py" line="228" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestHostsInfoCallback" name="test_master_only_nodes_are_ignored" file="test_opensearchpy/test_transport.py" line="109" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_add_connection" file="test_opensearchpy/test_transport.py" line="240" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_body_bytes_get_passed_untouched" file="test_opensearchpy/test_transport.py" line="203" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_body_gets_encoded_into_bytes" file="test_opensearchpy/test_transport.py" line="193" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_body_surrogates_replaced_encoded_into_bytes" file="test_opensearchpy/test_transport.py" line="211" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_custom_connection_class" file="test_opensearchpy/test_transport.py" line="231" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_failed_connection_will_be_marked_as_dead" file="test_opensearchpy/test_transport.py" line="258" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_kwargs_passed_on_to_connection_pool" file="test_opensearchpy/test_transport.py" line="226" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_kwargs_passed_on_to_connections" file="test_opensearchpy/test_transport.py" line="221" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_opaque_id" file="test_opensearchpy/test_transport.py" line="143" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_request_timeout_extracted_from_params_and_passed" file="test_opensearchpy/test_transport.py" line="132" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_request_will_fail_after_X_retries" file="test_opensearchpy/test_transport.py" line="249" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_request_with_custom_user_agent_header" file="test_opensearchpy/test_transport.py" line="163" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_resurrected_connection_will_be_marked_as_live_on_success" file="test_opensearchpy/test_transport.py" line="267" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_send_get_body_as_post" file="test_opensearchpy/test_transport.py" line="186" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_send_get_body_as_source" file="test_opensearchpy/test_transport.py" line="177" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_single_connection_uses_dummy_connection_pool" file="test_opensearchpy/test_transport.py" line="126" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_7x_publish_host" file="test_opensearchpy/test_transport.py" line="381" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_after_n_seconds" file="test_opensearchpy/test_transport.py" line="363" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_on_fail_failing_does_not_prevent_retires" file="test_opensearchpy/test_transport.py" line="345" time="0.002" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_on_fail_triggers_sniffing_on_fail" file="test_opensearchpy/test_transport.py" line="332" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_on_start_fetches_and_uses_nodes_list" file="test_opensearchpy/test_transport.py" line="287" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_on_start_ignores_sniff_timeout" file="test_opensearchpy/test_transport.py" line="296" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_reuses_connection_instances_if_possible" file="test_opensearchpy/test_transport.py" line="320" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_uses_sniff_timeout" file="test_opensearchpy/test_transport.py" line="308" time="0.001" /><testcase classname="test_opensearchpy.test_transport.TestTransport" name="test_sniff_will_use_seed_connections" file="test_opensearchpy/test_transport.py" line="279" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_ssl_context" file="test_opensearchpy/test_async/test_connection.py" line="76" time="0.010" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_opaque_id" file="test_opensearchpy/test_async/test_connection.py" line="92" time="0.001" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_no_http_compression" file="test_opensearchpy/test_async/test_connection.py" line="96" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_http_compression" file="test_opensearchpy/test_async/test_connection.py" line="109" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_url_prefix" file="test_opensearchpy/test_async/test_connection.py" line="134" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_default_user_agent" file="test_opensearchpy/test_async/test_connection.py" line="146" time="0.001" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_timeout_set" file="test_opensearchpy/test_async/test_connection.py" line="153" time="0.001" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_keep_alive_is_on_by_default" file="test_opensearchpy/test_async/test_connection.py" line="157" time="0.001" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_http_auth" file="test_opensearchpy/test_async/test_connection.py" line="165" time="0.001" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_http_auth_tuple" file="test_opensearchpy/test_async/test_connection.py" line="174" time="0.001" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_http_auth_list" file="test_opensearchpy/test_async/test_connection.py" line="183" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_uses_https_if_verify_certs_is_off" file="test_opensearchpy/test_async/test_connection.py" line="192" time="0.016" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_nowarn_when_test_uses_https_if_verify_certs_is_off" file="test_opensearchpy/test_async/test_connection.py" line="205" time="0.015" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_doesnt_use_https_if_not_specified" file="test_opensearchpy/test_async/test_connection.py" line="215" time="0.001" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_no_warning_when_using_ssl_context" file="test_opensearchpy/test_async/test_connection.py" line="219" time="0.008" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_warns_if_using_non_default_ssl_kwargs_with_ssl_context" file="test_opensearchpy/test_async/test_connection.py" line="225" time="0.042" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_uncompressed_body_logged" file="test_opensearchpy/test_async/test_connection.py" line="247" time="0.004" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_surrogatepass_into_bytes" file="test_opensearchpy/test_async/test_connection.py" line="258" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_recursion_error_reraised[RecursionError]" file="test_opensearchpy/test_async/test_connection.py" line="264" time="0.003" /><testcase classname="test_opensearchpy.test_async.test_connection.TestAIOHttpConnection" name="test_recursion_error_reraised[CancelledError]" file="test_opensearchpy/test_async/test_connection.py" line="264" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_connection.TestConnectionHttpbin" name="test_aiohttp_connection" file="test_opensearchpy/test_async/test_connection.py" line="290" time="1.168" /><testcase classname="test_opensearchpy.test_async.test_connection.TestConnectionHttpbin" name="test_aiohttp_connection_error" file="test_opensearchpy/test_async/test_connection.py" line="350" time="0.004" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_single_connection_uses_dummy_connection_pool" file="test_opensearchpy/test_async/test_transport.py" line="121" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_request_timeout_extracted_from_params_and_passed" file="test_opensearchpy/test_async/test_transport.py" line="129" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_opaque_id" file="test_opensearchpy/test_async/test_transport.py" line="141" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_request_with_custom_user_agent_header" file="test_opensearchpy/test_async/test_transport.py" line="163" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_send_get_body_as_source" file="test_opensearchpy/test_async/test_transport.py" line="176" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_send_get_body_as_post" file="test_opensearchpy/test_async/test_transport.py" line="185" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_body_gets_encoded_into_bytes" file="test_opensearchpy/test_async/test_transport.py" line="194" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_body_bytes_get_passed_untouched" file="test_opensearchpy/test_async/test_transport.py" line="206" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_body_surrogates_replaced_encoded_into_bytes" file="test_opensearchpy/test_async/test_transport.py" line="214" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_kwargs_passed_on_to_connections" file="test_opensearchpy/test_async/test_transport.py" line="226" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_kwargs_passed_on_to_connection_pool" file="test_opensearchpy/test_async/test_transport.py" line="232" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_custom_connection_class" file="test_opensearchpy/test_async/test_transport.py" line="238" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_add_connection" file="test_opensearchpy/test_async/test_transport.py" line="248" time="0.003" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_request_will_fail_after_X_retries" file="test_opensearchpy/test_async/test_transport.py" line="255" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_failed_connection_will_be_marked_as_dead" file="test_opensearchpy/test_async/test_transport.py" line="270" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_resurrected_connection_will_be_marked_as_live_on_success" file="test_opensearchpy/test_async/test_transport.py" line="285" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_will_use_seed_connections" file="test_opensearchpy/test_async/test_transport.py" line="298" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_on_start_fetches_and_uses_nodes_list" file="test_opensearchpy/test_async/test_transport.py" line="307" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_on_start_ignores_sniff_timeout" file="test_opensearchpy/test_async/test_transport.py" line="319" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_uses_sniff_timeout" file="test_opensearchpy/test_async/test_transport.py" line="333" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_reuses_connection_instances_if_possible" file="test_opensearchpy/test_async/test_transport.py" line="346" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_on_fail_triggers_sniffing_on_fail" file="test_opensearchpy/test_async/test_transport.py" line="360" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_on_fail_failing_does_not_prevent_retires" file="test_opensearchpy/test_async/test_transport.py" line="382" time="0.010" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_after_n_seconds" file="test_opensearchpy/test_async/test_transport.py" line="401" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_7x_publish_host" file="test_opensearchpy/test_async/test_transport.py" line="422" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_transport_close_closes_all_pool_connections" file="test_opensearchpy/test_async/test_transport.py" line="438" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_on_start_error_if_no_sniffed_hosts" file="test_opensearchpy/test_async/test_transport.py" line="453" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_on_start_waits_for_sniff_to_complete" file="test_opensearchpy/test_async/test_transport.py" line="470" time="1.003" /><testcase classname="test_opensearchpy.test_async.test_transport.TestTransport" name="test_sniff_on_start_close_unlocks_async_calls" file="test_opensearchpy/test_async/test_transport.py" line="506" time="0.002" /><testcase classname="test_opensearchpy.test_async.test_server.test_clients.TestUnicode" name="test_indices_analyze" file="test_opensearchpy/test_async/test_server/test_clients.py" line="35" time="0.051"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb84bb0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb91ba0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7e3b42c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7eb93b20&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03b[z\xcf\x10\xd8^\xb3\x93\x08\xa0\xf2\x81V/\xf9\xc0\xfe(\x08\x0f\x9e*\x0eN\xfbh...2\x06\xee\xac\x96\xa4\xa0h\xfa*pF\x83\xef\xf4\xa4\xfa-\x14,\xec\x95Q]?\xff\x7f\xb6\x1d(\xe0\xf73\xbcb\x1d\x8d\xfa7_m1='

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7eb91c90&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03b[z\xcf\x10\xd8^\xb3\x93\x08\xa0\xf2\x81V/\xf9\xc0\xfe(\x08\x0f\x9e*\x0eN\xfbh...2\x06\xee\xac\x96\xa4\xa0h\xfa*pF\x83\xef\xf4\xa4\xfa-\x14,\xec\x95Q]?\xff\x7f\xb6\x1d(\xe0\xf73\xbcb\x1d\x8d\xfa7_m1='
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7eb92080&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7eb91b40&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7eb91600&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb84bb0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb91ba0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb84bb0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb91ba0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb84bb0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb91ba0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb84bb0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb91ba0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb84bb0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb91ba0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7eb32890&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb32590&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb32590&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_indices_analyze&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7e380040&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7e383880&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_clients.TestBulk" name="test_bulk_works_with_string_body" file="test_opensearchpy/test_async/test_server/test_clients.py" line="40" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d190b50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1928f0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7df0ccc0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d1928c0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03&amp;|9&lt;auf\xb3\xc6 \xaf\x83\x14\x1bL\x11\x91\x7fa5\xbc\xad(P\xff\x07\xb6)w\xc4C\x...9a\xc6#M8!\xa91=\xd6\xce\x15]\xee&amp;\xb5\xc50\x9c\xd8Ni;\xd9\xd4\x94\xa6\xf3A\xed\x88\xf3\x0e73\xcbTY\xfc,d\'\'\x14F\x8a'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d1927a0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03&amp;|9&lt;auf\xb3\xc6 \xaf\x83\x14\x1bL\x11\x91\x7fa5\xbc\xad(P\xff\x07\xb6)w\xc4C\x...9a\xc6#M8!\xa91=\xd6\xce\x15]\xee&amp;\xb5\xc50\x9c\xd8Ni;\xd9\xd4\x94\xa6\xf3A\xed\x88\xf3\x0e73\xcbTY\xfc,d\'\'\x14F\x8a'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d192680&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d1929e0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d193dc0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d190b50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1928f0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d190b50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1928f0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d190b50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1928f0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d190b50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1928f0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d190b50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1928f0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d190ee0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d191630&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d191630&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_bulk_works_with_string_body&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7e383be0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7e3837f0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_clients.TestBulk" name="test_bulk_works_with_bytestring_body" file="test_opensearchpy/test_async/test_server/test_clients.py" line="47" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd42b90&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd418d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d5e1f40&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd417b0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xf4F\xc5\xc1\x1f\xb0\xe4@6\x96\xed?a@\xbcYU\xefs7\xac\xe0\x8b[ ,\xf1x4f\xaf\x...fa\x10\x97\\\xc7R\x9c\xae\x97\'\xd8\xc4\xf7?Oor\xa3\xd4\xcb\xb7\xb4\x91\xd9U;V\x8ca\x98\x02\x19\xf6\x0e\xc4Y.\xcd\xafS'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd41840&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xf4F\xc5\xc1\x1f\xb0\xe4@6\x96\xed?a@\xbcYU\xefs7\xac\xe0\x8b[ ,\xf1x4f\xaf\x...fa\x10\x97\\\xc7R\x9c\xae\x97\'\xd8\xc4\xf7?Oor\xa3\xd4\xcb\xb7\xb4\x91\xd9U;V\x8ca\x98\x02\x19\xf6\x0e\xc4Y.\xcd\xafS'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd41ed0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd41a80&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd42b30&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd42b90&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd418d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd42b90&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd418d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd42b90&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd418d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd42b90&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd418d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd42b90&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd418d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd43220&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd43be0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd43be0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_bulk_works_with_bytestring_body&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7ebafd00&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7e3831c0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_clients.TestYarlMissing" name="test_aiohttp_connection_works_without_yarl" file="test_opensearchpy/test_async/test_server/test_clients.py" line="56" time="0.048"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd69660&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd68550&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d52c4c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd6bdc0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x96\x0c\xbb\xbc4\x1b\x04\x06\xda\xbd\xa3\xbcs;y\x04z\x03\xf0\xb1/\xb1M$\xad\x...a03\xe7\xd8p\x1f\xf8I?}\xbb\xaac\xdd\xd2\xdfpm\xf2\xfb/2\\\xa0K\xcc{_@\xf7\xcc3\xd6\xc0\xd1\x81\xfc\xa179?\xe4\xc6\x07'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd68070&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x96\x0c\xbb\xbc4\x1b\x04\x06\xda\xbd\xa3\xbcs;y\x04z\x03\xf0\xb1/\xb1M$\xad\x...a03\xe7\xd8p\x1f\xf8I?}\xbb\xaac\xdd\xd2\xdfpm\xf2\xfb/2\\\xa0K\xcc{_@\xf7\xcc3\xd6\xc0\xd1\x81\xfc\xa179?\xe4\xc6\x07'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd6be50&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd68430&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd69510&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd69660&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd68550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd69660&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd68550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd69660&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd68550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd69660&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd68550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd69660&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd68550&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd6b580&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd6b7c0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd6b7c0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_aiohttp_connection_works_without_yarl&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7e383760&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d5e7760&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_actions_remain_unchanged" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="67" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cdf92a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cdfa5c0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7df9a7c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cdfa830&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x037Ua&lt;\xb9|+\x95\xd4(\xc6Qg=f\xe4\xd2e\x82\xe1\x95r\xe3\xa8\x9d\xfdb\xbb\xfd2?l ...xa4\xc6\x90\xfd6\xfdJL\xe4\x14\xc0\x9b3\x8aIV\xda_\x00\x022\x98\xbc\xfc\xa2\xc1\xaaa} {;\xe9\xe3!\x17\xa4V\xde\xd4\xb3'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cdfa500&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x037Ua&lt;\xb9|+\x95\xd4(\xc6Qg=f\xe4\xd2e\x82\xe1\x95r\xe3\xa8\x9d\xfdb\xbb\xfd2?l ...xa4\xc6\x90\xfd6\xfdJL\xe4\x14\xc0\x9b3\x8aIV\xda_\x00\x022\x98\xbc\xfc\xa2\xc1\xaaa} {;\xe9\xe3!\x17\xa4V\xde\xd4\xb3'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cdfadd0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cdfa3e0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cdf9480&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cdf92a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cdfa5c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cdf92a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cdfa5c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cdf92a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cdfa5c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cdf92a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cdfa5c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cdf92a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cdfa5c0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cdf8a00&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cdf8340&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cdf8340&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_actions_remain_unchanged&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7e3c9f30&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d5e7d00&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_all_documents_get_inserted" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="75" time="0.048"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7dfbeb30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7dfbd8d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d52fec0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7dfbef20&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x1e\x08\x0b\xa9\xa6\x8c\xb4\xf1\x9a\xc7\x92&gt;\xfe\xbc^\xb9\x1fK\xdd\xef8\x99\x...8dH\xec\xd6\xf5\xa15\x89\xb1\x96&amp;\xb5.\xa8\x89\x13\xb3w|\n\xbc3\x16\xbd\xf5\x1a\xe5\xddKG\x95i1jM\xd1#\\\x155Q\x0b\xb2'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7dfbd1e0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x1e\x08\x0b\xa9\xa6\x8c\xb4\xf1\x9a\xc7\x92&gt;\xfe\xbc^\xb9\x1fK\xdd\xef8\x99\x...8dH\xec\xd6\xf5\xa15\x89\xb1\x96&amp;\xb5.\xa8\x89\x13\xb3w|\n\xbc3\x16\xbd\xf5\x1a\xe5\xddKG\x95i1jM\xd1#\\\x155Q\x0b\xb2'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7dfbda80&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7dfbd060&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7dfbd930&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7dfbeb30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7dfbd8d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7dfbeb30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7dfbd8d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7dfbeb30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7dfbd8d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7dfbeb30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7dfbd8d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7dfbeb30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7dfbd8d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7dfbf790&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb387c0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb387c0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_all_documents_get_inserted&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d5e6a70&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d5e56c0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_documents_data_types" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="87" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47a30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd46260&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d58b0c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd460e0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03d\xe5\x1e\x0e\xef\x8eQ\xb6\x9f\xb1\x16F\x1f -&lt;A\xae\xfb\x18H\xed&gt;\xe2\\\xee\\\...x90\xa7*\xf6T\xcabK\x85\xf0g\x8a\x84[\x98\xf1\x85_\xea\xda\x05H\x05{\x9a]\xa5\x96\xa3\x10\x05\xddp`\xda&amp;\xb7u\xeb\xcdZ'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd461d0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03d\xe5\x1e\x0e\xef\x8eQ\xb6\x9f\xb1\x16F\x1f -&lt;A\xae\xfb\x18H\xed&gt;\xe2\\\xee\\\...x90\xa7*\xf6T\xcabK\x85\xf0g\x8a\x84[\x98\xf1\x85_\xea\xda\x05H\x05{\x9a]\xa5\x96\xa3\x10\x05\xddp`\xda&amp;\xb7u\xeb\xcdZ'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd45630&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd463b0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd476d0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47a30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd46260&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47a30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd46260&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47a30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd46260&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47a30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd46260&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47a30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd46260&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd47d60&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd443d0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd443d0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_documents_data_types&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7df45d80&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7df476d0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_all_errors_from_chunk_are_raised_on_failure" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="121" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d112b60&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d110a30&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d5298c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d1110f0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xdd\xb8fp\x8b\x07H\x96W\xfbn\xfc\x8bJQn\x16\xe3 \xc3RoI3\xc3:\xf0L\x9a\xb2\xb...\xb8\x10\xceO\xbf\xb0\x9e\xcb\xa5\xe5\xf6]\xe0t\xd4\xf8C\xd0a\x10\xe9\\\xf1\x18\xf9\xa5\xfd\xc0\xe5\x06k&lt;\xe7\x82\xffi'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d112b90&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xdd\xb8fp\x8b\x07H\x96W\xfbn\xfc\x8bJQn\x16\xe3 \xc3RoI3\xc3:\xf0L\x9a\xb2\xb...\xb8\x10\xceO\xbf\xb0\x9e\xcb\xa5\xe5\xf6]\xe0t\xd4\xf8C\xd0a\x10\xe9\\\xf1\x18\xf9\xa5\xfd\xc0\xe5\x06k&lt;\xe7\x82\xffi'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d111630&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d110c40&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d113010&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d112b60&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d110a30&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d112b60&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d110a30&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d112b60&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d110a30&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d112b60&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d110a30&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d112b60&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d110a30&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d113d00&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d113130&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d113130&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_all_errors_from_chunk_are_raised_on_failure&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7df47520&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7df47130&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_different_op_types" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="141" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd809d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd81a20&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d58a740&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd81330&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03IE\xbcY\xb8\xe5\xeb\xb6l\xf5S\xce\x12g\xf3\x18\xf1\xfa\xe6\x87\x04\x8bQ6/0\xea...\x9bg\xc4\x18\xd8X\x91\x18I\x18$\xd1\x86\xba\xc4\xcc\xe7\xd4_}\xcfx\x82Bp\x9f\xf8 \xb7\x84LzG\nx\xaez\x9b\x92\xa3&lt;Ruyr'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd82ce0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03IE\xbcY\xb8\xe5\xeb\xb6l\xf5S\xce\x12g\xf3\x18\xf1\xfa\xe6\x87\x04\x8bQ6/0\xea...\x9bg\xc4\x18\xd8X\x91\x18I\x18$\xd1\x86\xba\xc4\xcc\xe7\xd4_}\xcfx\x82Bp\x9f\xf8 \xb7\x84LzG\nx\xaez\x9b\x92\xa3&lt;Ruyr'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd83d30&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd82920&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd80340&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd809d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd81a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd809d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd81a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd809d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd81a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd809d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd81a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd809d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd81a20&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd81960&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd82290&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd82290&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_different_op_types&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7df47640&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7df47eb0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_transport_error_can_becaught" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="156" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb31480&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d559120&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cd5de40&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d558640&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03~\x90\x94P{\xcf\x86\r\\Y\xd1\x1a\x82cq&gt;\xd7\x0e\xa4&gt;\xdf|n\x14r\x14\xbf\xb4\xe...\xd3\xaf\x8c\xb59\x07\x99_}v,!\xcd\x96\xc9\xac\xa1|c\xb7\xbe?\x7fR\xe0\nEg\xe0OZ&amp;o\xc5\x8cF\xfct\x8d\xda*\xbcG\xb9\xdb'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d558100&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03~\x90\x94P{\xcf\x86\r\\Y\xd1\x1a\x82cq&gt;\xd7\x0e\xa4&gt;\xdf|n\x14r\x14\xbf\xb4\xe...\xd3\xaf\x8c\xb59\x07\x99_}v,!\xcd\x96\xc9\xac\xa1|c\xb7\xbe?\x7fR\xe0\nEg\xe0OZ&amp;o\xc5\x8cF\xfct\x8d\xda*\xbcG\xb9\xdb'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d5592a0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d55a710&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7eb305e0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb31480&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d559120&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb31480&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d559120&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb31480&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d559120&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb31480&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d559120&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb31480&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d559120&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7eb303a0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb30eb0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb30eb0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_transport_error_can_becaught&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7df47490&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d1e9cf0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_rejected_documents_are_retried" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="190" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1ca2c0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1c9b10&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d1ba840&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d1c9300&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03[\x08E\x93\xf1&lt;\xf5\x02\xf2j\x0b\xaa\xf4\x81K\x80M\xeb\xf8\xe1\x17xoZ6\xc3&lt;$@C...3\xcf\x96-\x08\xfd\x12\x91\xd20\xcb\xf8\x01\x95\xc24v4\x99\x91Y^?\xe0\x03\x17\xa6\x8b\x1f\xf1&amp;\x042\xecvr\xa8\x84\xb6_'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d1c9120&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03[\x08E\x93\xf1&lt;\xf5\x02\xf2j\x0b\xaa\xf4\x81K\x80M\xeb\xf8\xe1\x17xoZ6\xc3&lt;$@C...3\xcf\x96-\x08\xfd\x12\x91\xd20\xcb\xf8\x01\x95\xc24v4\x99\x91Y^?\xe0\x03\x17\xa6\x8b\x1f\xf1&amp;\x042\xecvr\xa8\x84\xb6_'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d1ca4a0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d1ca7a0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d1ca2f0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1ca2c0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1c9b10&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1ca2c0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1c9b10&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1ca2c0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1c9b10&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1ca2c0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1c9b10&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1ca2c0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d1c9b10&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d1ca470&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d1c9360&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d1c9360&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_rejected_documents_are_retried&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d1e9480&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d1eb520&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_rejected_documents_are_retried_at_most_max_retries_times" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="218" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d507a60&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7d1ba1c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d506b00&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xad\xea\xc3\xcf\x92\xber?6\x1fG\xaf\xe5\xaa"\x99d\xe7uM\x8e\xff\x81\xab\x8f\x...bbw\x16\xd32\xa7\xde\x9d\xf9l\x82\xe5\xc5\xe9`\x14f^t\xf8[7\xa2b\xe4\x0f~X\xb1_\xab\x83\x17Z\xae\x93Mc\xbb\xda\x86\x9f'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d506200&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xad\xea\xc3\xcf\x92\xber?6\x1fG\xaf\xe5\xaa"\x99d\xe7uM\x8e\xff\x81\xab\x8f\x...bbw\x16\xd32\xa7\xde\x9d\xf9l\x82\xe5\xc5\xe9`\x14f^t\xf8[7\xa2b\xe4\x0f~X\xb1_\xab\x83\x17Z\xae\x93Mc\xbb\xda\x86\x9f'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d504b50&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d5074c0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d507970&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d507a60&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d507a60&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d507a60&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d507a60&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d507a60&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d5077f0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d505cc0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d505cc0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_rejected_documents_are_retried_at_most_max_retries_times&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d1ea0e0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d1eab90&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestStreamingBulk" name="test_transport_error_is_raised_with_max_retries" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="249" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9e9b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9db70&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cd142c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7eb9ed70&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xdc\xc0\xcc^\x81\xa7\xcd\x1aM`K\xde\xfc\xd4\xa2\xdb1\xdc\xff\xcf\xdfTi\x14\x0...\xab3\xa60\x8ak\x95\x0e\xcacdl\xe3\x9fV\xddv\x87\xc8z\x8a\xcb\xa7\xa2\x1c\xba\x8bZ\xaf4z\xfcF\x87#\x17\x8bd\xb7,aX\xd8'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7eb9eef0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xdc\xc0\xcc^\x81\xa7\xcd\x1aM`K\xde\xfc\xd4\xa2\xdb1\xdc\xff\xcf\xdfTi\x14\x0...\xab3\xa60\x8ak\x95\x0e\xcacdl\xe3\x9fV\xddv\x87\xc8z\x8a\xcb\xa7\xa2\x1c\xba\x8bZ\xaf4z\xfcF\x87#\x17\x8bd\xb7,aX\xd8'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7eb9e170&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7eb9d2d0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7eb9d180&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9e9b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9db70&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9e9b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9db70&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9e9b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9db70&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9e9b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9db70&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9e9b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9db70&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7eb9c850&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb9f430&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb9f430&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_transport_error_is_raised_with_max_retries&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d1e9090&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d1ebac0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestBulk" name="test_bulk_works_with_single_item" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="275" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507d30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5047c0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cda38c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7c106770&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x8b+\xc3j\x03\x98\xe9\xa9J\r\x12A\xa7\t\xe1\xf4\x82\x14\xe3\x19\tA\xbbvs\xc5\...f3wF\xfax\xcb2\xba\xf2\xe6\x18\xc1\xd6z\x7fr\x8f\x13\x91\xd0[\x94,g\x1b\x14T\xc1mA\xa9g\x13T/\xe6\xfbp\xa8eFp\x8a\x8bZ'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7c1065f0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x8b+\xc3j\x03\x98\xe9\xa9J\r\x12A\xa7\t\xe1\xf4\x82\x14\xe3\x19\tA\xbbvs\xc5\...f3wF\xfax\xcb2\xba\xf2\xe6\x18\xc1\xd6z\x7fr\x8f\x13\x91\xd0[\x94,g\x1b\x14T\xc1mA\xa9g\x13T/\xe6\xfbp\xa8eFp\x8a\x8bZ'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7c106620&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d504250&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d5070a0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507d30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5047c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507d30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5047c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507d30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5047c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507d30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5047c0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d507d30&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5047c0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d504700&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d507be0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d507be0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_bulk_works_with_single_item&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d1eac20&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d1e9a20&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestBulk" name="test_all_documents_get_inserted" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="288" time="0.048"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c141fc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c141390&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cda3440&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7c143d60&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03Anch\xa7\xaa\xcf\xae\x9b\xafUq\xbd\xa0\xd4\x9anA\x14\xe5p&amp;\x8a\x9b\x19\x9aA\xa...82.p\xeb-\x0b\xd2Q\xc4-n\xa4v\xb0\xee\x97Ir\x9al_\x96\x9e\x11{\x00\xad\xb8+\x87A&amp;\x80\x11m\xa3\xfe\x1f\xde\xbdz!\xf5r-'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7c142800&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03Anch\xa7\xaa\xcf\xae\x9b\xafUq\xbd\xa0\xd4\x9anA\x14\xe5p&amp;\x8a\x9b\x19\x9aA\xa...82.p\xeb-\x0b\xd2Q\xc4-n\xa4v\xb0\xee\x97Ir\x9al_\x96\x9e\x11{\x00\xad\xb8+\x87A&amp;\x80\x11m\xa3\xfe\x1f\xde\xbdz!\xf5r-'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7c141360&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7c141e40&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7c141a50&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c141fc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c141390&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c141fc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c141390&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c141fc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c141390&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c141fc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c141390&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c141fc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c141390&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7c141bd0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d504400&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d504400&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_all_documents_get_inserted&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d92e560&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d92fac0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestBulk" name="test_stats_only_reports_numbers" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="301" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d10ebc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10e320&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cdf3ec0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d10d8a0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x99Z*\x02\xa2\x92\x08\x9a\xe6@\xc6\x857\x17\xba3\xeb\xd1\x84\xf4\xea\xb8\xb9e...0\xfd\xd1\xd1\x94\x9eJ0\xf3Y\xad#\x8a[\xb0\x84+\xa7\x12\xe0\xdf\xa4e\xc0\xfa\xc6\xac\x86\xd4\xd5F\r\x9b:\xd8"\xb7\x1du'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d10ea70&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x99Z*\x02\xa2\x92\x08\x9a\xe6@\xc6\x857\x17\xba3\xeb\xd1\x84\xf4\xea\xb8\xb9e...0\xfd\xd1\xd1\x94\x9eJ0\xf3Y\xad#\x8a[\xb0\x84+\xa7\x12\xe0\xdf\xa4e\xc0\xfa\xc6\xac\x86\xd4\xd5F\r\x9b:\xd8"\xb7\x1du'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d10c040&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d10d360&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d10ffd0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d10ebc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10e320&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d10ebc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10e320&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d10ebc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10e320&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d10ebc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10e320&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d10ebc0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10e320&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d10c8e0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d10ebf0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d10ebf0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_stats_only_reports_numbers&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d92f6d0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d92fd90&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestBulk" name="test_errors_are_reported_correctly" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="311" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd7b0a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd78250&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cdf1d40&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd78a30&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03Bma\xaa`\x84\xfd\x06E\xa6\x9a\xa9r\xf2NV\xb8\xe8\x0c\xb7J0\x11p\xec\x8d&gt;\xfa\x...\xcd\xce\xedK\xc58&amp;42j\xe5-k\x11\x87\xb7\x7f\xadH/\xcd\xab\xc3\xfbe\xb5\x1d\xd6\rGH\xac\xc1\xcd\x8f-l\x95)\xeb\xf1\xd6'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd786d0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03Bma\xaa`\x84\xfd\x06E\xa6\x9a\xa9r\xf2NV\xb8\xe8\x0c\xb7J0\x11p\xec\x8d&gt;\xfa\x...\xcd\xce\xedK\xc58&amp;42j\xe5-k\x11\x87\xb7\x7f\xadH/\xcd\xab\xc3\xfbe\xb5\x1d\xd6\rGH\xac\xc1\xcd\x8f-l\x95)\xeb\xf1\xd6'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd79720&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd7b880&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd7b5e0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd7b0a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd78250&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd7b0a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd78250&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd7b0a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd78250&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd7b0a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd78250&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd7b0a0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd78250&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd795d0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd7a470&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd7a470&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_errors_are_reported_correctly&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d92e7a0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7cdca200&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestBulk" name="test_error_is_raised" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="337" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7d5b10&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7d7730&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cdf30c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7e7d68f0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03x\n7Ld\xfc2\x0e\xc9T\x17\x981\x9a\xfb\xf3BA\x92\xa6\x8a\x07\x812\xd5\xd9\xee\x...y\xb47\xf4\'P\xd4/\x8aw\xf2\x0b\xe9\x8fY\xa4\x13\xde|\xd3G\xc3\x8c&lt;\xd6\xc82\x8f@\xe7.l\xa7uvk\xaf3\xf8o\xd7\x94XI\xde'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7e7d6380&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03x\n7Ld\xfc2\x0e\xc9T\x17\x981\x9a\xfb\xf3BA\x92\xa6\x8a\x07\x812\xd5\xd9\xee\x...y\xb47\xf4\'P\xd4/\x8aw\xf2\x0b\xe9\x8fY\xa4\x13\xde|\xd3G\xc3\x8c&lt;\xd6\xc82\x8f@\xe7.l\xa7uvk\xaf3\xf8o\xd7\x94XI\xde'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7e7d67a0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7e7d63e0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7e7d5f00&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7d5b10&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7d7730&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7d5b10&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7d7730&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7d5b10&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7d7730&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7d5b10&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7d7730&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7d5b10&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7d7730&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7e7d4d90&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd7bd90&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd7bd90&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_error_is_raised&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7d92e680&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7cdcb880&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestBulk" name="test_ignore_error_if_raised" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="350" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e9630&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7ea3e0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7cdf3ec0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7e7ea0b0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\\\x014\xfd\xf9\xbd\x9d\xfbX\xa9^-e]&gt;lk\xfbs6\x0f\xc2_6\x91\xfb\n\xc3b\xb5)e  ...x1b\xcb\x90\xad\x075\x80$\xb7%Y\xf0\xca\x9eyGG\xe4\xb6B\x0b\x16\x9aPXQ\xd3\x1d\xb4\xae\xfb\x80O\x0e\x00\x10\xb1\xe5m[s'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7e7e8100&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\\\x014\xfd\xf9\xbd\x9d\xfbX\xa9^-e]&gt;lk\xfbs6\x0f\xc2_6\x91\xfb\n\xc3b\xb5)e  ...x1b\xcb\x90\xad\x075\x80$\xb7%Y\xf0\xca\x9eyGG\xe4\xb6B\x0b\x16\x9aPXQ\xd3\x1d\xb4\xae\xfb\x80O\x0e\x00\x10\xb1\xe5m[s'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7e7ebdf0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7e7ea080&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7e7eaec0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e9630&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7ea3e0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e9630&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7ea3e0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e9630&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7ea3e0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e9630&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7ea3e0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e9630&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7ea3e0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7e7ea020&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7ea050&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7ea050&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_ignore_error_if_raised&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7cdcb400&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7cdcb640&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestBulk" name="test_errors_are_collected_properly" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="383" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47760&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd45330&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c1e2cc0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd44370&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x8f{\xee\xcc\x03\x17\xfb\xe3\xf0\xbc\x97\x8et`\xee"\xa4\xc8\x07\xa7!U\x93\x1d...b6\xeb\x03\x0c\x005\x0em\xecr\x97\x1b\xe6\x96\xdfM\x84G\x80\xc5\xc4m\xabGi/\xe4d\xa5\xacE\xcf\xf1E\xfa\xbejE\xddX\x1b.'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd440d0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x8f{\xee\xcc\x03\x17\xfb\xe3\xf0\xbc\x97\x8et`\xee"\xa4\xc8\x07\xa7!U\x93\x1d...b6\xeb\x03\x0c\x005\x0em\xecr\x97\x1b\xe6\x96\xdfM\x84G\x80\xc5\xc4m\xabGi/\xe4d\xa5\xacE\xcf\xf1E\xfa\xbejE\xddX\x1b.'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd451e0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd47130&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd45a50&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47760&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd45330&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47760&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd45330&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47760&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd45330&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47760&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd45330&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd47760&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd45330&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd44d30&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7eaf80&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7eaf80&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_errors_are_collected_properly&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7cdcbeb0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7d92eb90&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_order_can_be_preserved" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="444" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e8e50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7e8070&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c13ff40&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7e7eb7f0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xe9\xe6\xb4\xb3"\xd1-\xb9\xb1~7\x8d9i)XD)\xaci\x85\xf2lo\xbe\x8db\x9dqQ\xaak ...xef\x84\x8c\x1aD\xa6\x11a\x01\x9d*\x1d\x9an\xbfv\xac\x06\xfc\x94\xb6\xe0I\xe6C&gt;\xd0E\xe6\xe9\xfa\x13Bb\x18\xdd\xd8\xa0'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7e7e8ee0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xe9\xe6\xb4\xb3"\xd1-\xb9\xb1~7\x8d9i)XD)\xaci\x85\xf2lo\xbe\x8db\x9dqQ\xaak ...xef\x84\x8c\x1aD\xa6\x11a\x01\x9d*\x1d\x9an\xbfv\xac\x06\xfc\x94\xb6\xe0I\xe6C&gt;\xd0E\xe6\xe9\xfa\x13Bb\x18\xdd\xd8\xa0'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7e7ea170&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7e7eb880&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7e7eacb0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e8e50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7e8070&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e8e50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7e8070&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e8e50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7e8070&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e8e50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7e8070&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7e7e8e50&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7e7e8070&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7e7eb2e0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7e91e0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7e91e0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_order_can_be_preserved&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c1d40d0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c1d5ab0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_all_documents_are_read" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="465" time="0.048"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7efed5d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7efed2a0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c1e2740&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7efee290&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x86\x06\xbb\xc3\xd7$\xa6\x856\xbf\xdfm\xcc=\x0cI&gt;(\xa0{\xdb\x1cgx\xff\xcc\x87...\xactl\x1b\x13\xe9\x0f\xce\xf8\xd5*\x95\xfd:R\xe2\xde\x9a\x8cbON\x85j.n\xe6\xaf\xa1\tH]\x08\xc0\xc4~\x0e:\xe9k\x86\xd7'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7efee2f0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x86\x06\xbb\xc3\xd7$\xa6\x856\xbf\xdfm\xcc=\x0cI&gt;(\xa0{\xdb\x1cgx\xff\xcc\x87...\xactl\x1b\x13\xe9\x0f\xce\xf8\xd5*\x95\xfd:R\xe2\xde\x9a\x8cbON\x85j.n\xe6\xaf\xa1\tH]\x08\xc0\xc4~\x0e:\xe9k\x86\xd7'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7efee350&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7efee080&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7efee6e0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7efed5d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7efed2a0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7efed5d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7efed2a0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7efed5d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7efed2a0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7efed5d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7efed2a0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7efed5d0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7efed2a0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7efec370&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7ebd60&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7e7ebd60&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_all_documents_are_read&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c1d6cb0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c1d6d40&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_scroll_error" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="481" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd73490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd70e50&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c984340&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd73730&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03t\xe95v\xff\xc2\xce\xa5\xe1\xb4\x8b1\xe2\xa93}\xbdq\xd6\xbe\t.\x9c\xa6\xa1\xc7...r\xf6/x\x85\xe9z\xa9W\xc8u\x1a\xb4\x99\x9f\xf7\x1aX\xa4m\x0b\xfe\xbe0\xd7\xa5\xbb\xca\xb7\x1a\x145\xabx=\xd9\xa5^.\xe8'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd70d60&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03t\xe95v\xff\xc2\xce\xa5\xe1\xb4\x8b1\xe2\xa93}\xbdq\xd6\xbe\t.\x9c\xa6\xa1\xc7...r\xf6/x\x85\xe9z\xa9W\xc8u\x1a\xb4\x99\x9f\xf7\x1aX\xa4m\x0b\xfe\xbe0\xd7\xa5\xbb\xca\xb7\x1a\x145\xabx=\xd9\xa5^.\xe8'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd716c0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd73280&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd732b0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd73490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd70e50&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd73490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd70e50&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd73490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd70e50&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd73490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd70e50&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd73490&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd70e50&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd71660&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd72da0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd72da0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_scroll_error&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7cdcb640&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c1d4280&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_initial_search_error" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="517" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9cf40&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9cca0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c13ec40&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7eb9f310&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x18"\xe2\xb5\xda\xf8\xc6\xd9~\x15\x0b\x01O\x00\xd9**\xbc=\xad\xa6\xc4\x17\x08...0(h\xf0\x07;\xac\x08{h\xe1\x84@\x04\r\x17-\x9eb2Rw6\x95\x04\xcc\xad\xa1\x96\xf1\'\x14&amp;(\xf2B\x16\x93\x89\'\x10\x11\xac'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7eb9dbd0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x18"\xe2\xb5\xda\xf8\xc6\xd9~\x15\x0b\x01O\x00\xd9**\xbc=\xad\xa6\xc4\x17\x08...0(h\xf0\x07;\xac\x08{h\xe1\x84@\x04\r\x17-\x9eb2Rw6\x95\x04\xcc\xad\xa1\x96\xf1\'\x14&amp;(\xf2B\x16\x93\x89\'\x10\x11\xac'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7eb9f0d0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7eb9eec0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7eb9e470&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9cf40&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9cca0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9cf40&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9cca0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9cf40&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9cca0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9cf40&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9cca0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7eb9cf40&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7eb9cca0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7eb9c280&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb9e6e0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7eb9e6e0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_initial_search_error&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c1d7370&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c196ef0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_no_scroll_id_fast_route" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="569" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c1523b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c150550&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c987740&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7c150b80&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xaa\xceC\xae.L\x0f\xf7t\xe1\x17g\x8bZ\xdb+/\xbe\xc8\xfe8\xe4\x98\xef3(\xad"\x...\xf5\xe9j4\r&amp;\xcb\x82u`\x83-\x11/\xe8\xb8%\xab\x16\x05\xf8\xc2Y1Z!\x07t\x1282\t\x17\xe3O!\xcd2\x9f\x86\xdf\xcb\xe4\xb7'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7c150370&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xaa\xceC\xae.L\x0f\xf7t\xe1\x17g\x8bZ\xdb+/\xbe\xc8\xfe8\xe4\x98\xef3(\xad"\x...\xf5\xe9j4\r&amp;\xcb\x82u`\x83-\x11/\xe8\xb8%\xab\x16\x05\xf8\xc2Y1Z!\x07t\x1282\t\x17\xe3O!\xcd2\x9f\x86\xdf\xcb\xe4\xb7'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7c150880&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7c150ca0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7c151780&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c1523b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c150550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c1523b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c150550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c1523b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c150550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c1523b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c150550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c1523b0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c150550&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7c1521a0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7c151420&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7c151420&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_no_scroll_id_fast_route&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c196b00&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c197f40&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_logger" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="584" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd86440&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd86050&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c958440&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd84eb0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x038\x9e\xf5\xfaW\xbd]\x91\x82\xc3Y4\xed\xf8\x8d\xb4u\xbe\xce\xb9\x9d\xddt@\x8a\x...xb7~Lc7\xd38\x9aqq\xa3D\x99~4\x90\xae7&gt;5K\xcc\x95\\13\xe4\xd3t&gt;\x9a{f\xb1r\xbc&lt;\xb3\xef\xcf/JNr\xa3w\xcc\xecty\x17\xab'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd87340&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x038\x9e\xf5\xfaW\xbd]\x91\x82\xc3Y4\xed\xf8\x8d\xb4u\xbe\xce\xb9\x9d\xddt@\x8a\x...xb7~Lc7\xd38\x9aqq\xa3D\x99~4\x90\xae7&gt;5K\xcc\x95\\13\xe4\xd3t&gt;\x9a{f\xb1r\xbc&lt;\xb3\xef\xcf/JNr\xa3w\xcc\xecty\x17\xab'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd86cb0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd84070&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd86e60&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd86440&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd86050&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd86440&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd86050&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd86440&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd86050&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd86440&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd86050&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd86440&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd86050&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd84b20&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd840d0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd840d0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_logger&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c194ca0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c197e20&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_clear_scroll" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="626" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5b1de0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5b1180&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c970140&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d5b0490&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xc8\xd7r7\xf4\x1a\xcb\x986a\xa8\\zI\xfc9\xfe\xb5@jw\xc1LN#\x99\xaeu\x12\xd1LL...\x0f\xd4\x8aH\xa4ZD\xfeE\x02\xbe\xdba\xc0\xe0\xe0\xa7\x94\x9f~\xf2y\x96\xebSJi\x1d\xb9,\x85\x9f^I\xd4\xe9fgka\xa6\x143'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d5b2050&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xc8\xd7r7\xf4\x1a\xcb\x986a\xa8\\zI\xfc9\xfe\xb5@jw\xc1LN#\x99\xaeu\x12\xd1LL...\x0f\xd4\x8aH\xa4ZD\xfeE\x02\xbe\xdba\xc0\xe0\xe0\xa7\x94\x9f~\xf2y\x96\xebSJi\x1d\xb9,\x85\x9f^I\xd4\xe9fgka\xa6\x143'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d5b14e0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d5b00d0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d5b1480&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5b1de0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5b1180&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5b1de0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5b1180&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5b1de0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5b1180&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5b1de0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5b1180&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5b1de0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5b1180&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d5b1690&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d5b3520&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d5b3520&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_clear_scroll&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c196710&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c1d5cf0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_scan_auth_kwargs_forwarded[kwargs0]" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="662" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd990f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd98550&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c9717c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd9ae00&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x8az\xc1\x15\x9d&gt;6\x97f$\xdf\xe9\xa7\xd0\x84\x85*\xfc\xd5E\x1b\xaeW=.\xd9=\xc..._5]a5\xef\xa0\xeb\x10 \x94\x1d\t6\x92\xc0\xd20\xb1\x1c\xc5\xc5/\x1f@c\'\xae\xf8\xc2S\x0bY6\x12"\xd8\x8ez\x9c\xf1\xd1\r'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd9a4a0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x8az\xc1\x15\x9d&gt;6\x97f$\xdf\xe9\xa7\xd0\x84\x85*\xfc\xd5E\x1b\xaeW=.\xd9=\xc..._5]a5\xef\xa0\xeb\x10 \x94\x1d\t6\x92\xc0\xd20\xb1\x1c\xc5\xc5/\x1f@c\'\xae\xf8\xc2S\x0bY6\x12"\xd8\x8ez\x9c\xf1\xd1\r'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd9b520&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd99b10&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd9b490&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd990f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd98550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd990f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd98550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd990f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd98550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd990f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd98550&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd990f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd98550&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd9a830&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd9b610&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd9b610&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_scan_auth_kwargs_forwarded[kwargs0]&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c9c7760&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c9c7d00&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_scan_auth_kwargs_forwarded[kwargs1]" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="662" time="0.050"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9f5a80&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7effc160&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7c9897c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7effd4e0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x9c\xfc\xf23[\xf5\xb8+B\xdbn\x86\x059\xd5X\xb2\xc1\xace\xea\xaa\xa75\xbft1\x7...fa|\xbe`R\xc6\n!\xae?\x8c\\P\xf9\r%{\x86\xfe\xacd+5\xd6\x18A\xfcj\x98T\xd7"\xc9U6@\xd8\xb0\x85\xaa\xf1\xbd\xf2\x99\x9b'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7effdc60&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\x9c\xfc\xf23[\xf5\xb8+B\xdbn\x86\x059\xd5X\xb2\xc1\xace\xea\xaa\xa75\xbft1\x7...fa|\xbe`R\xc6\n!\xae?\x8c\\P\xf9\r%{\x86\xfe\xacd+5\xd6\x18A\xfcj\x98T\xd7"\xc9U6@\xd8\xb0\x85\xaa\xf1\xbd\xf2\x99\x9b'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7effed40&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7effdde0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d9f6410&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9f5a80&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7effc160&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9f5a80&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7effc160&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9f5a80&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7effc160&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9f5a80&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7effc160&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9f5a80&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7effc160&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d9f4640&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d9f6cb0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d9f6cb0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_scan_auth_kwargs_forwarded[kwargs1]&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c9c75b0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c9c7250&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_scan_auth_kwargs_forwarded[kwargs2]" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="662" time="0.055"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d91a140&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d919a20&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7bd4c540&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d918a00&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03)T\xe1\xfanE\xb7,&lt;~\x9d(\x89\xac\x01\xa3\xc6\x8e\xd6X\xe6\xa1\x11=\xa8\x8e\x06...x81/\xb8\xa7\xcehn\x04\x96\x07R\xd1\x9f\xc7\x9f\x1fh\xdd\x1fjBn\xc12 \x80\x04+k9\x97z=hwZ\x80ik\x0ci\x1a 1\x051/$:\x80'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d91ab90&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03)T\xe1\xfanE\xb7,&lt;~\x9d(\x89\xac\x01\xa3\xc6\x8e\xd6X\xe6\xa1\x11=\xa8\x8e\x06...x81/\xb8\xa7\xcehn\x04\x96\x07R\xd1\x9f\xc7\x9f\x1fh\xdd\x1fjBn\xc12 \x80\x04+k9\x97z=hwZ\x80ik\x0ci\x1a 1\x051/$:\x80'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d91aa70&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d9195a0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d919b70&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d91a140&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d919a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d91a140&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d919a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d91a140&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d919a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d91a140&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d919a20&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d91a140&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d919a20&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d9198a0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d91a710&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d91a710&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_scan_auth_kwargs_forwarded[kwargs2]&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c9c77f0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c9c68c0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestScan" name="test_scan_auth_kwargs_favor_scroll_kwargs_option" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="712" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c12b880&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c128be0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7b9c6540&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7c12b910&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03RO\xc4\x80if\xf7\xa1\xb6\xeb\xddeU2!0\xdd?\xc0\xe7h&amp;|\xe6\xd8\x94}\xfb\xc29\xb...xf6\xf7\xd4\xc1[W3\x00\xad\xf4\x8f\x88|\xb2\xc1\xeb\x0b\x0cXIPW\xf2v\xd2\'\x04Vr\xe6L\x00\x85\x83\xc7\x17\x8c~\xa9\xe7'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7c128f70&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03RO\xc4\x80if\xf7\xa1\xb6\xeb\xddeU2!0\xdd?\xc0\xe7h&amp;|\xe6\xd8\x94}\xfb\xc29\xb...xf6\xf7\xd4\xc1[W3\x00\xad\xf4\x8f\x88|\xb2\xc1\xeb\x0b\x0cXIPW\xf2v\xd2\'\x04Vr\xe6L\x00\x85\x83\xc7\x17\x8c~\xa9\xe7'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7c12bf40&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7c128b80&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7c12b820&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c12b880&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c128be0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c12b880&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c128be0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c12b880&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c128be0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c12b880&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c128be0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7c12b880&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7c128be0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7c129150&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7c128f10&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7c128f10&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_scan_auth_kwargs_favor_scroll_kwargs_option&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c97ae60&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c97b6d0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestReindex" name="test_reindex_passes_kwargs_to_scan_and_bulk" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="779" time="0.051"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5af700&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5ae1d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7b9c5ac0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d5af340&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03`\xa2! h\xd7\x17\xef\xa6?c\xdaO\xe1\x1f"k]ni\x97\x10\xa2\xa6`H\t\x90\xc49\xae\...f7^\xa1\x7f\x07\xc5\x07\xa4\xfc\xba\t\r\x89P\x9c\x8b\xf7\r.D\xe12D?\xec\x8e\x14\xd1\x19S\xc0\x10"c\xe6\x8e\xeb\x04\xc2'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d5ad630&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03`\xa2! h\xd7\x17\xef\xa6?c\xdaO\xe1\x1f"k]ni\x97\x10\xa2\xa6`H\t\x90\xc49\xae\...f7^\xa1\x7f\x07\xc5\x07\xa4\xfc\xba\t\r\x89P\x9c\x8b\xf7\r.D\xe12D?\xec\x8e\x14\xd1\x19S\xc0\x10"c\xe6\x8e\xeb\x04\xc2'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d5aee90&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d5afeb0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d5ad2a0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5af700&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5ae1d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5af700&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5ae1d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5af700&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5ae1d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5af700&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5ae1d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d5af700&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d5ae1d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d5ad030&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d5ac160&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d5ac160&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_reindex_passes_kwargs_to_scan_and_bulk&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c979cf0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c9791b0&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestReindex" name="test_reindex_accepts_a_query" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="800" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1090f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10bdc0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7b927cc0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d10ba00&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03:Vc\xbe"OY\x06&lt;|\xe8\x94\xf1\x0eo2=\xa9b\xcd\xc5|\xb3\x97\xf1\xef(\xf99;\x96\x...\xd8`F\xfd\x1b&lt;\x16\x99\x97\x1c\xbeW\xb1Id\x86J\xcbT\x84\xa3\xee6\xc9\xeat2\xadq\xef$\x9ef\x0e#E\xbe\xff\x03\xc3\r\xda'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d14b6d0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03:Vc\xbe"OY\x06&lt;|\xe8\x94\xf1\x0eo2=\xa9b\xcd\xc5|\xb3\x97\xf1\xef(\xf99;\x96\x...\xd8`F\xfd\x1b&lt;\x16\x99\x97\x1c\xbeW\xb1Id\x86J\xcbT\x84\xa3\xee6\xc9\xeat2\xadq\xef$\x9ef\x0e#E\xbe\xff\x03\xc3\r\xda'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d14a5f0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d10b340&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d10b430&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1090f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10bdc0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1090f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10bdc0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1090f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10bdc0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1090f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10bdc0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d1090f0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d10bdc0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d10a9b0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d10afe0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d10afe0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_reindex_accepts_a_query&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c9781f0&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7c979d80&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestReindex" name="test_all_documents_get_moved" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="819" time="0.049"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9c2dd0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d9c27d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7b9ad9c0&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7d9c18a0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xb8\x11\xbcN\xa5\xe03\xc8\x80\x0eex\xb0\xd2t\xa3\xa6\x10\x1a\xc6\xef\xf8\xd3g...cf6\x83b\xe8"N\x92\xdaF-&gt;\x19x\t\x1b\xa6f"m\xbe"\x0e\xd6\x9b\x1a\xce\xbe\xb4\x1aR=\xb0\x0e\xf21\x7f\xc2\xef\x05OwG\xf5'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7d560df0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xb8\x11\xbcN\xa5\xe03\xc8\x80\x0eex\xb0\xd2t\xa3\xa6\x10\x1a\xc6\xef\xf8\xd3g...cf6\x83b\xe8"N\x92\xdaF-&gt;\x19x\t\x1b\xa6f"m\xbe"\x0e\xd6\x9b\x1a\xce\xbe\xb4\x1aR=\xb0\x0e\xf21\x7f\xc2\xef\x05OwG\xf5'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7d562da0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7d9c28f0&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7d9c25f0&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9c2dd0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d9c27d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9c2dd0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d9c27d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9c2dd0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d9c27d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9c2dd0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d9c27d0&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7d9c2dd0&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7d9c27d0&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7d9c0f40&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d9c37c0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7d9c37c0&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;, request = &lt;SubRequest 'async_client' for &lt;Function test_all_documents_get_moved&gt;&gt;
kwargs = {}, setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7c978670&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7bdb2d40&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_async.test_server.test_helpers.TestParentChildReindex" name="test_children_are_reindexed_correctly" file="test_opensearchpy/test_async/test_server/test_helpers.py" line="869" time="0.048"><error message="failed on setup with &quot;opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')&quot;">self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd83100&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd80490&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
&gt;               return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa

venv/lib/python3.10/site-packages/aiohttp/connector.py:986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
host = '127.0.0.1', port = 9200

    async def create_connection(
            self, protocol_factory, host=None, port=None,
            *, ssl=None, family=0,
            proto=0, flags=0, sock=None,
            local_addr=None, server_hostname=None,
            ssl_handshake_timeout=None,
            happy_eyeballs_delay=None, interleave=None):
        """Connect to a TCP server.
    
        Create a streaming transport connection to a given internet host and
        port: socket family AF_INET or socket.AF_INET6 depending on host (or
        family if specified), socket type SOCK_STREAM. protocol_factory must be
        a callable returning a protocol instance.
    
        This method is a coroutine which will try to establish the connection
        in the background.  When successful, the coroutine returns a
        (transport, protocol) pair.
        """
        if server_hostname is not None and not ssl:
            raise ValueError('server_hostname is only meaningful with ssl')
    
        if server_hostname is None and ssl:
            # Use host as default for server_hostname.  It is an error
            # if host is empty or not set, e.g. when an
            # already-connected socket was passed or when only a port
            # is given.  To avoid this error, you can pass
            # server_hostname='' -- this will bypass the hostname
            # check.  (This also means that if host is a numeric
            # IP/IPv6 address, we will attempt to verify that exact
            # address; this will probably fail, but it is possible to
            # create a certificate for a specific IP address, so we
            # don't judge it here.)
            if not host:
                raise ValueError('You must set server_hostname '
                                 'when using ssl without a host')
            server_hostname = host
    
        if ssl_handshake_timeout is not None and not ssl:
            raise ValueError(
                'ssl_handshake_timeout is only meaningful with ssl')
    
        if sock is not None:
            _check_ssl_socket(sock)
    
        if happy_eyeballs_delay is not None and interleave is None:
            # If using happy eyeballs, default to interleave addresses by family
            interleave = 1
    
        if host is not None or port is not None:
            if sock is not None:
                raise ValueError(
                    'host/port and sock can not be specified at the same time')
    
            infos = await self._ensure_resolved(
                (host, port), family=family,
                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)
            if not infos:
                raise OSError('getaddrinfo() returned empty list')
    
            if local_addr is not None:
                laddr_infos = await self._ensure_resolved(
                    local_addr, family=family,
                    type=socket.SOCK_STREAM, proto=proto,
                    flags=flags, loop=self)
                if not laddr_infos:
                    raise OSError('getaddrinfo() returned empty list')
            else:
                laddr_infos = None
    
            if interleave:
                infos = _interleave_addrinfos(infos, interleave)
    
            exceptions = []
            if happy_eyeballs_delay is None:
                # not using happy eyeballs
                for addrinfo in infos:
                    try:
                        sock = await self._connect_sock(
                            exceptions, addrinfo, laddr_infos)
                        break
                    except OSError:
                        continue
            else:  # using happy eyeballs
                sock, _, _ = await staggered.staggered_race(
                    (functools.partial(self._connect_sock,
                                       exceptions, addrinfo, laddr_infos)
                     for addrinfo in infos),
                    happy_eyeballs_delay, loop=self)
    
            if sock is None:
                exceptions = [exc for sub in exceptions for exc in sub]
                if len(exceptions) == 1:
                    raise exceptions[0]
                else:
                    # If they all have the same str(), raise one.
                    model = str(exceptions[0])
                    if all(str(exc) == model for exc in exceptions):
                        raise exceptions[0]
                    # Raise a combined exception so the user can see all
                    # the various error messages.
                    raise OSError('Multiple exceptions: {}'.format(
                        ', '.join(str(exc) for exc in exceptions)))
    
        else:
            if sock is None:
                raise ValueError(
                    'host and port was not specified and no sock specified')
            if sock.type != socket.SOCK_STREAM:
                # We allow AF_INET, AF_INET6, AF_UNIX as long as they
                # are SOCK_STREAM.
                # We support passing AF_UNIX sockets even though we have
                # a dedicated API for that: create_unix_connection.
                # Disallowing AF_UNIX in this method, breaks backwards
                # compatibility.
                raise ValueError(
                    f'A Stream Socket was expected, got {sock!r}')
    
&gt;       transport, protocol = await self._create_connection_transport(
            sock, protocol_factory, ssl, server_hostname,
            ssl_handshake_timeout=ssl_handshake_timeout)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1089: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
sock = &lt;socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6&gt;
protocol_factory = functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;)
ssl = &lt;ssl.SSLContext object at 0x7f5c7b9af540&gt;, server_hostname = 'localhost', server_side = False, ssl_handshake_timeout = None

    async def _create_connection_transport(
            self, sock, protocol_factory, ssl,
            server_hostname, server_side=False,
            ssl_handshake_timeout=None):
    
        sock.setblocking(False)
    
        protocol = protocol_factory()
        waiter = self.create_future()
        if ssl:
            sslcontext = None if isinstance(ssl, bool) else ssl
            transport = self._make_ssl_transport(
                sock, protocol, sslcontext, waiter,
                server_side=server_side, server_hostname=server_hostname,
                ssl_handshake_timeout=ssl_handshake_timeout)
        else:
            transport = self._make_socket_transport(sock, protocol, waiter)
    
        try:
&gt;           await waiter

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:1119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto.SSLProtocol object at 0x7f5c7cd81ae0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xdb\x89\n\xde\x01J\x18exSy7%D\x14\x9a\xc2\xcf_\xec)5@\xc4\xfc\xa0N\xb3\xfc#F\...xe0\x12\xf4\x91&amp;"\xfem\xb2\xa7\xdf\xa1\xf7\xffk[&amp;\xde\xc0\xca\xab(3\xfb\xc1\x1d\xbecA\xba\x9aI\x8c\x1bY\xb8\xa8+\x8a.}'

    def data_received(self, data):
        """Called when some SSL data is received.
    
        The argument is a bytes object.
        """
        if self._sslpipe is None:
            # transport closing, sslpipe is destroyed
            return
    
        try:
&gt;           ssldata, appdata = self._sslpipe.feed_ssldata(data)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;asyncio.sslproto._SSLPipe object at 0x7f5c7cd82ce0&gt;
data = b'\x16\x03\x03\x00z\x02\x00\x00v\x03\x03\xdb\x89\n\xde\x01J\x18exSy7%D\x14\x9a\xc2\xcf_\xec)5@\xc4\xfc\xa0N\xb3\xfc#F\...xe0\x12\xf4\x91&amp;"\xfem\xb2\xa7\xdf\xa1\xf7\xffk[&amp;\xde\xc0\xca\xab(3\xfb\xc1\x1d\xbecA\xba\x9aI\x8c\x1bY\xb8\xa8+\x8a.}'
only_handshake = False

    def feed_ssldata(self, data, only_handshake=False):
        """Feed SSL record level data into the pipe.
    
        The data must be a bytes instance. It is OK to send an empty bytes
        instance. This can be used to get ssldata for a handshake initiated by
        this endpoint.
    
        Return a (ssldata, appdata) tuple. The ssldata element is a list of
        buffers containing SSL data that needs to be sent to the remote SSL.
    
        The appdata element is a list of buffers containing plaintext data that
        needs to be forwarded to the application. The appdata list may contain
        an empty buffer indicating an SSL "close_notify" alert. This alert must
        be acknowledged by calling shutdown().
        """
        if self._state == _UNWRAPPED:
            # If unwrapped, pass plaintext data straight through.
            if data:
                appdata = [data]
            else:
                appdata = []
            return ([], appdata)
    
        self._need_ssldata = False
        if data:
            self._incoming.write(data)
    
        ssldata = []
        appdata = []
        try:
            if self._state == _DO_HANDSHAKE:
                # Call do_handshake() until it doesn't raise anymore.
&gt;               self._sslobj.do_handshake()

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/sslproto.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;ssl.SSLObject object at 0x7f5c7cd830a0&gt;

    def do_handshake(self):
        """Start the SSL/TLS handshake."""
&gt;       self._sslobj.do_handshake()
E       ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)

/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/ssl.py:975: SSLCertVerificationError

The above exception was the direct cause of the following exception:

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
&gt;           async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:

opensearchpy/_async/http_aiohttp.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client._RequestContextManager object at 0x7f5c7cd82710&gt;

    async def __aenter__(self) -&gt; _RetType:
&gt;       self._resp = await self._coro

venv/lib/python3.10/site-packages/aiohttp/client.py:1138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.client.ClientSession object at 0x7f5c7cd80460&gt;, method = 'GET', str_or_url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')

    async def _request(
        self,
        method: str,
        str_or_url: StrOrURL,
        *,
        params: Optional[Mapping[str, str]] = None,
        data: Any = None,
        json: Any = None,
        cookies: Optional[LooseCookies] = None,
        headers: Optional[LooseHeaders] = None,
        skip_auto_headers: Optional[Iterable[str]] = None,
        auth: Optional[BasicAuth] = None,
        allow_redirects: bool = True,
        max_redirects: int = 10,
        compress: Optional[str] = None,
        chunked: Optional[bool] = None,
        expect100: bool = False,
        raise_for_status: Optional[bool] = None,
        read_until_eof: bool = True,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        timeout: Union[ClientTimeout, object] = sentinel,
        verify_ssl: Optional[bool] = None,
        fingerprint: Optional[bytes] = None,
        ssl_context: Optional[SSLContext] = None,
        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        trace_request_ctx: Optional[SimpleNamespace] = None,
        read_bufsize: Optional[int] = None,
    ) -&gt; ClientResponse:
    
        # NOTE: timeout clamps existing connect and read timeouts.  We cannot
        # set the default to None because we need to detect if the user wants
        # to use the existing timeouts by setting timeout to None.
    
        if self.closed:
            raise RuntimeError("Session is closed")
    
        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)
    
        if data is not None and json is not None:
            raise ValueError(
                "data and json parameters can not be used at the same time"
            )
        elif json is not None:
            data = payload.JsonPayload(json, dumps=self._json_serialize)
    
        if not isinstance(chunked, bool) and chunked is not None:
            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)
    
        redirects = 0
        history = []
        version = self._version
    
        # Merge with default headers and transform to CIMultiDict
        headers = self._prepare_headers(headers)
        proxy_headers = self._prepare_headers(proxy_headers)
    
        try:
            url = self._build_url(str_or_url)
        except ValueError as e:
            raise InvalidURL(str_or_url) from e
    
        skip_headers = set(self._skip_auto_headers)
        if skip_auto_headers is not None:
            for i in skip_auto_headers:
                skip_headers.add(istr(i))
    
        if proxy is not None:
            try:
                proxy = URL(proxy)
            except ValueError as e:
                raise InvalidURL(proxy) from e
    
        if timeout is sentinel:
            real_timeout = self._timeout  # type: ClientTimeout
        else:
            if not isinstance(timeout, ClientTimeout):
                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]
            else:
                real_timeout = timeout
        # timeout is cumulative for all request operations
        # (request, redirects, responses, data consuming)
        tm = TimeoutHandle(self._loop, real_timeout.total)
        handle = tm.start()
    
        if read_bufsize is None:
            read_bufsize = self._read_bufsize
    
        traces = [
            Trace(
                self,
                trace_config,
                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),
            )
            for trace_config in self._trace_configs
        ]
    
        for trace in traces:
            await trace.send_request_start(method, url.update_query(params), headers)
    
        timer = tm.timer()
        try:
            with timer:
                while True:
                    url, auth_from_url = strip_auth_from_url(url)
                    if auth and auth_from_url:
                        raise ValueError(
                            "Cannot combine AUTH argument with "
                            "credentials encoded in URL"
                        )
    
                    if auth is None:
                        auth = auth_from_url
                    if auth is None:
                        auth = self._default_auth
                    # It would be confusing if we support explicit
                    # Authorization header with auth argument
                    if (
                        headers is not None
                        and auth is not None
                        and hdrs.AUTHORIZATION in headers
                    ):
                        raise ValueError(
                            "Cannot combine AUTHORIZATION header "
                            "with AUTH argument or credentials "
                            "encoded in URL"
                        )
    
                    all_cookies = self._cookie_jar.filter_cookies(url)
    
                    if cookies is not None:
                        tmp_cookie_jar = CookieJar()
                        tmp_cookie_jar.update_cookies(cookies)
                        req_cookies = tmp_cookie_jar.filter_cookies(url)
                        if req_cookies:
                            all_cookies.load(req_cookies)
    
                    if proxy is not None:
                        proxy = URL(proxy)
                    elif self._trust_env:
                        with suppress(LookupError):
                            proxy, proxy_auth = get_env_proxy_for_url(url)
    
                    req = self._request_class(
                        method,
                        url,
                        params=params,
                        headers=headers,
                        skip_auto_headers=skip_headers,
                        data=data,
                        cookies=all_cookies,
                        auth=auth,
                        version=version,
                        compress=compress,
                        chunked=chunked,
                        expect100=expect100,
                        loop=self._loop,
                        response_class=self._response_class,
                        proxy=proxy,
                        proxy_auth=proxy_auth,
                        timer=timer,
                        session=self,
                        ssl=ssl,
                        proxy_headers=proxy_headers,
                        traces=traces,
                    )
    
                    # connection timeout
                    try:
                        async with ceil_timeout(real_timeout.connect):
                            assert self._connector is not None
&gt;                           conn = await self._connector.connect(
                                req, traces=traces, timeout=real_timeout
                            )

venv/lib/python3.10/site-packages/aiohttp/client.py:535: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd83100&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd80490&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def connect(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; Connection:
        """Get from pool or create new connection."""
        key = req.connection_key
        available = self._available_connections(key)
    
        # Wait if there are no available connections or if there are/were
        # waiters (i.e. don't steal connection from a waiter about to wake up)
        if available &lt;= 0 or key in self._waiters:
            fut = self._loop.create_future()
    
            # This connection will now count towards the limit.
            self._waiters[key].append(fut)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_start()
    
            try:
                await fut
            except BaseException as e:
                if key in self._waiters:
                    # remove a waiter even if it was cancelled, normally it's
                    #  removed when it's notified
                    try:
                        self._waiters[key].remove(fut)
                    except ValueError:  # fut may no longer be in list
                        pass
    
                raise e
            finally:
                if key in self._waiters and not self._waiters[key]:
                    del self._waiters[key]
    
            if traces:
                for trace in traces:
                    await trace.send_connection_queued_end()
    
        proto = self._get(key)
        if proto is None:
            placeholder = cast(ResponseHandler, _TransportPlaceholder())
            self._acquired.add(placeholder)
            self._acquired_per_host[key].add(placeholder)
    
            if traces:
                for trace in traces:
                    await trace.send_connection_create_start()
    
            try:
&gt;               proto = await self._create_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd83100&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd80490&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_connection(
        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"
    ) -&gt; ResponseHandler:
        """Create connection.
    
        Has same keyword arguments as BaseEventLoop.create_connection.
        """
        if req.proxy:
            _, proto = await self._create_proxy_connection(req, traces, timeout)
        else:
&gt;           _, proto = await self._create_direct_connection(req, traces, timeout)

venv/lib/python3.10/site-packages/aiohttp/connector.py:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd83100&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd80490&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
                transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )
            except ClientConnectorError as exc:
                last_exc = exc
                continue
    
            if req.is_ssl() and fingerprint:
                try:
                    fingerprint.check(transp)
                except ServerFingerprintMismatch as exc:
                    transp.close()
                    if not self._cleanup_closed_disabled:
                        self._cleanup_closed_transports.append(transp)
                    last_exc = exc
                    continue
    
            return transp, proto
        else:
            assert last_exc is not None
&gt;           raise last_exc

venv/lib/python3.10/site-packages/aiohttp/connector.py:1206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd83100&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd80490&gt;, traces = []
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None)

    async def _create_direct_connection(
        self,
        req: "ClientRequest",
        traces: List["Trace"],
        timeout: "ClientTimeout",
        *,
        client_error: Type[Exception] = ClientConnectorError,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        sslcontext = self._get_ssl_context(req)
        fingerprint = self._get_fingerprint(req)
    
        host = req.url.raw_host
        assert host is not None
        port = req.port
        assert port is not None
        host_resolved = asyncio.ensure_future(
            self._resolve_host(host, port, traces=traces), loop=self._loop
        )
        try:
            # Cancelling this lookup should not cancel the underlying lookup
            #  or else the cancel event will get broadcast to all the waiters
            #  across all connections.
            hosts = await asyncio.shield(host_resolved)
        except asyncio.CancelledError:
    
            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -&gt; None:
                with suppress(Exception, asyncio.CancelledError):
                    fut.result()
    
            host_resolved.add_done_callback(drop_exception)
            raise
        except OSError as exc:
            # in case of proxy it is not ClientProxyConnectionError
            # it is problem of resolving proxy ip itself
            raise ClientConnectorError(req.connection_key, exc) from exc
    
        last_exc = None  # type: Optional[Exception]
    
        for hinfo in hosts:
            host = hinfo["host"]
            port = hinfo["port"]
    
            try:
&gt;               transp, proto = await self._wrap_create_connection(
                    self._factory,
                    host,
                    port,
                    timeout=timeout,
                    ssl=sslcontext,
                    family=hinfo["family"],
                    proto=hinfo["proto"],
                    flags=hinfo["flags"],
                    server_hostname=hinfo["hostname"] if sslcontext else None,
                    local_addr=self._local_addr,
                    req=req,
                    client_error=client_error,
                )

venv/lib/python3.10/site-packages/aiohttp/connector.py:1175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;aiohttp.connector.TCPConnector object at 0x7f5c7cd83100&gt;, req = &lt;aiohttp.client_reqrep.ClientRequest object at 0x7f5c7cd80490&gt;
timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), client_error = &lt;class 'aiohttp.client_exceptions.ClientConnectorError'&gt;
args = (functools.partial(&lt;class 'aiohttp.client_proto.ResponseHandler'&gt;, loop=&lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;), '127.0.0.1', 9200)
kwargs = {'family': &lt;AddressFamily.AF_INET: 2&gt;, 'flags': &lt;AddressInfo.AI_NUMERICSERV|AI_NUMERICHOST: 1028&gt;, 'local_addr': None, 'proto': 6, ...}

    async def _wrap_create_connection(
        self,
        *args: Any,
        req: "ClientRequest",
        timeout: "ClientTimeout",
        client_error: Type[Exception] = ClientConnectorError,
        **kwargs: Any,
    ) -&gt; Tuple[asyncio.Transport, ResponseHandler]:
        try:
            async with ceil_timeout(timeout.sock_connect):
                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa
        except cert_errors as exc:
&gt;           raise ClientConnectorCertificateError(req.connection_key, exc) from exc
E           aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]

venv/lib/python3.10/site-packages/aiohttp/connector.py:988: ClientConnectorCertificateError

During handling of the above exception, another exception occurred:

    @pytest.fixture(scope="function")
    async def async_client():
        client = None
        try:
            if not hasattr(opensearchpy, "AsyncOpenSearch"):
                pytest.skip("test requires 'AsyncOpenSearch'")
    
            kw = {"timeout": 3}
            client = opensearchpy.AsyncOpenSearch(OPENSEARCH_URL, **kw)
    
            # wait for yellow status
            for _ in range(100):
                try:
&gt;                   await client.cluster.health(wait_for_status="yellow")

test_opensearchpy/test_async/test_server/conftest.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.client.cluster.ClusterClient object at 0x7f5c7cd80ca0&gt;, index = None, params = {'wait_for_status': b'yellow'}, headers = {}

    @query_params(
        "expand_wildcards",
        "level",
        "local",
        "master_timeout",
        "timeout",
        "wait_for_active_shards",
        "wait_for_events",
        "wait_for_no_initializing_shards",
        "wait_for_no_relocating_shards",
        "wait_for_nodes",
        "wait_for_status",
    )
    async def health(self, index=None, params=None, headers=None):
        """
        Returns basic information about the health of the cluster.
    
    
        :arg index: Limit the information returned to a specific index
        :arg expand_wildcards: Whether to expand wildcard expression to
            concrete indices that are open, closed or both.  Valid choices: open,
            closed, hidden, none, all  Default: all
        :arg level: Specify the level of detail for returned information
            Valid choices: cluster, indices, shards  Default: cluster
        :arg local: Return local information, do not retrieve the state
            from master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection
            to master node
        :arg timeout: Explicit operation timeout
        :arg wait_for_active_shards: Wait until the specified number of
            shards is active
        :arg wait_for_events: Wait until all currently queued events
            with the given priority are processed  Valid choices: immediate, urgent,
            high, normal, low, languid
        :arg wait_for_no_initializing_shards: Whether to wait until
            there are no initializing shards in the cluster
        :arg wait_for_no_relocating_shards: Whether to wait until there
            are no relocating shards in the cluster
        :arg wait_for_nodes: Wait until the specified number of nodes is
            available
        :arg wait_for_status: Wait until cluster is in a specific state
            Valid choices: green, yellow, red
        """
&gt;       return await self.transport.perform_request(
            "GET",
            _make_path("_cluster", "health", index),
            params=params,
            headers=headers,
        )

opensearchpy/_async/client/cluster.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd81900&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
                status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )
    
                # Lowercase all the header names for consistency in accessing them.
                headers_response = {
                    header.lower(): value for header, value in headers_response.items()
                }
            except TransportError as e:
                if method == "HEAD" and e.status_code == 404:
                    return False
    
                retry = False
                if isinstance(e, ConnectionTimeout):
                    retry = self.retry_on_timeout
                elif isinstance(e, ConnectionError):
                    retry = True
                elif e.status_code in self.retry_on_status:
                    retry = True
    
                if retry:
                    try:
                        # only mark as dead if we are retrying
                        self.mark_dead(connection)
                    except TransportError:
                        # If sniffing on failure, it could fail too. Catch the
                        # exception not to interrupt the retries.
                        pass
                    # raise exception on last retry
                    if attempt == self.max_retries:
&gt;                       raise e

opensearchpy/_async/transport.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;opensearchpy._async.transport.AsyncTransport object at 0x7f5c7cd81900&gt;, method = 'GET', url = '/_cluster/health', headers = {}
params = {'wait_for_status': b'yellow'}, body = None

    async def perform_request(self, method, url, headers=None, params=None, body=None):
        """
        Perform the actual request. Retrieve a connection from the connection
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the connection as failed and retry (up
        to `max_retries` times).
    
        If the operation was successful and the connection used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg url: absolute url (without host) to target
        :arg headers: dictionary of headers, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class
        :arg params: dictionary of query parameters, will be handed over to the
            underlying :class:`~opensearchpy.Connection` class for serialization
        :arg body: body of the request, will be serialized using serializer and
            passed to the connection
        """
        await self._async_call()
    
        method, params, body, ignore, timeout = self._resolve_request_args(
            method, params, body
        )
    
        for attempt in range(self.max_retries + 1):
            connection = self.get_connection()
    
            try:
&gt;               status, headers_response, data = await connection.perform_request(
                    method,
                    url,
                    params,
                    body,
                    headers=headers,
                    ignore=ignore,
                    timeout=timeout,
                )

opensearchpy/_async/transport.py:339: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;AIOHttpConnection: https://localhost:9200&gt;, method = 'GET', url = URL('https://localhost:9200/_cluster/health?wait_for_status=yellow')
params = {'wait_for_status': b'yellow'}, body = None, timeout = ClientTimeout(total=3, connect=None, sock_read=None, sock_connect=None), ignore = (), headers = {}

    async def perform_request(
        self, method, url, params=None, body=None, timeout=None, ignore=(), headers=None
    ):
        if self.session is None:
            await self._create_aiohttp_session()
        assert self.session is not None
    
        orig_body = body
        url_path = self.url_prefix + url
        if params:
            query_string = urlencode(params)
        else:
            query_string = ""
    
        # There is a bug in aiohttp that disables the re-use
        # of the connection in the pool when method=HEAD.
        # See: aio-libs/aiohttp#1769
        is_head = False
        if method == "HEAD":
            method = "GET"
            is_head = True
    
        # Top-tier tip-toeing happening here. Basically
        # because Pip's old resolver is bad and wipes out
        # strict pins in favor of non-strict pins of extras
        # our [async] extra overrides aiohttp's pin of
        # yarl. yarl released breaking changes, aiohttp pinned
        # defensively afterwards, but our users don't get
        # that nice pin that aiohttp set. :( So to play around
        # this super-defensively we try to import yarl, if we can't
        # then we pass a string into ClientSession.request() instead.
        if yarl:
            # Provide correct URL object to avoid string parsing in low-level code
            url = yarl.URL.build(
                scheme=self.scheme,
                host=self.hostname,
                port=self.port,
                path=url_path,
                query_string=query_string,
                encoded=True,
            )
        else:
            url = self.url_prefix + url
            if query_string:
                url = "%s?%s" % (url, query_string)
            url = self.host + url
    
        timeout = aiohttp.ClientTimeout(
            total=timeout if timeout is not None else self.timeout
        )
    
        req_headers = self.headers.copy()
        if headers:
            req_headers.update(headers)
    
        if self.http_compress and body:
            body = self._gzip_compress(body)
            req_headers["content-encoding"] = "gzip"
    
        start = self.loop.time()
        try:
            async with self.session.request(
                method,
                url,
                data=body,
                headers=req_headers,
                timeout=timeout,
                fingerprint=self.ssl_assert_fingerprint,
            ) as response:
                if is_head:  # We actually called 'GET' so throw away the data.
                    await response.release()
                    raw_data = ""
                else:
                    raw_data = await response.text()
                duration = self.loop.time() - start
    
        # We want to reraise a cancellation or recursion error.
        except reraise_exceptions:
            raise
        except Exception as e:
            self.log_request_fail(
                method,
                str(url),
                url_path,
                orig_body,
                self.loop.time() - start,
                exception=e,
            )
            if isinstance(e, aiohttp_exceptions.ServerFingerprintMismatch):
                raise SSLError("N/A", str(e), e)
            if isinstance(
                e, (asyncio.TimeoutError, aiohttp_exceptions.ServerTimeoutError)
            ):
                raise ConnectionTimeout("TIMEOUT", str(e), e)
&gt;           raise ConnectionError("N/A", str(e), e)
E           opensearchpy.exceptions.ConnectionError: ConnectionError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')]) caused by: ClientConnectorCertificateError(Cannot connect to host localhost:9200 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')])

opensearchpy/_async/http_aiohttp.py:322: ConnectionError

During handling of the above exception, another exception occurred:

event_loop = &lt;_UnixSelectorEventLoop running=False closed=False debug=False&gt;
request = &lt;SubRequest 'async_client' for &lt;Function test_children_are_reindexed_correctly&gt;&gt;, kwargs = {}
setup = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.setup at 0x7f5c7bdb2950&gt;
finalizer = &lt;function _wrap_asyncgen.&lt;locals&gt;._asyncgen_fixture_wrapper.&lt;locals&gt;.finalizer at 0x7f5c7bdb1870&gt;

    @functools.wraps(func)
    def _asyncgen_fixture_wrapper(
        event_loop: asyncio.AbstractEventLoop, request: SubRequest, **kwargs: Any
    ) -&gt; _R:
        gen_obj = func(**_add_kwargs(func, kwargs, event_loop, request))
    
        async def setup() -&gt; _R:
            res = await gen_obj.__anext__()
            return res
    
        def finalizer() -&gt; None:
            """Yield again, to finalize."""
    
            async def async_finalizer() -&gt; None:
                try:
                    await gen_obj.__anext__()
                except StopAsyncIteration:
                    pass
                else:
                    msg = "Async generator fixture didn't stop."
                    msg += "Yield only once."
                    raise ValueError(msg)
    
            event_loop.run_until_complete(async_finalizer())
    
&gt;       result = event_loop.run_until_complete(setup())

venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:293: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/linuxbrew/.linuxbrew/opt/python@3.10/lib/python3.10/asyncio/base_events.py:646: in run_until_complete
    return future.result()
venv/lib/python3.10/site-packages/pytest_asyncio/plugin.py:275: in setup
    res = await gen_obj.__anext__()
test_opensearchpy/test_async/test_server/conftest.py:64: in async_client
    wipe_cluster(client)
test_opensearchpy/utils.py:46: in wipe_cluster
    wipe_snapshots(client)
test_opensearchpy/utils.py:76: in wipe_snapshots
    repos = client.snapshot.get_repository(repository="_all")
opensearchpy/client/utils.py:177: in _wrapped
    return func(*args, params=params, headers=headers, **kwargs)
opensearchpy/client/snapshot.py:150: in get_repository
    return self.transport.perform_request(
opensearchpy/transport.py:407: in perform_request
    raise e
opensearchpy/transport.py:368: in perform_request
    status, headers_response, data = connection.perform_request(
opensearchpy/connection/http_urllib3.py:275: in perform_request
    self._raise_error(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpConnection: https://localhost:9200&gt;, status_code = 401, raw_data = 'Unauthorized', content_type = 'text/plain'

    def _raise_error(self, status_code, raw_data, content_type=None):
        """Locate appropriate exception and raise it."""
        error_message = raw_data
        additional_info = None
        try:
            content_type = (
                "text/plain"
                if content_type is None
                else content_type.split(";")[0].strip()
            )
            if raw_data and content_type == "application/json":
                additional_info = json.loads(raw_data)
                error_message = additional_info.get("error", error_message)
                if isinstance(error_message, dict) and "type" in error_message:
                    error_message = error_message["type"]
        except (ValueError, TypeError) as err:
            logger.warning("Undecodable raw error response from server: %s", err)
    
&gt;       raise HTTP_EXCEPTIONS.get(status_code, TransportError)(
            status_code, error_message, additional_info
        )
E       opensearchpy.exceptions.AuthenticationException: AuthenticationException(401, 'Unauthorized')

opensearchpy/connection/base.py:300: AuthenticationException</error></testcase><testcase classname="test_opensearchpy.test_client.test_cluster.TestCluster" name="test_state_with_index_without_metric_defaults_to_all" file="test_opensearchpy/test_client/test_cluster.py" line="41" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_cluster.TestCluster" name="test_stats_with_node_id" file="test_opensearchpy/test_client/test_cluster.py" line="34" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_cluster.TestCluster" name="test_stats_without_node_id" file="test_opensearchpy/test_client/test_cluster.py" line="30" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_indices.TestIndices" name="test_create_one_index" file="test_opensearchpy/test_client/test_indices.py" line="30" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_indices.TestIndices" name="test_delete_multiple_indices" file="test_opensearchpy/test_client/test_indices.py" line="34" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_indices.TestIndices" name="test_exists_index" file="test_opensearchpy/test_client/test_indices.py" line="38" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_indices.TestIndices" name="test_passing_empty_value_for_required_param_raises_exception" file="test_opensearchpy/test_client/test_indices.py" line="42" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_cluster_state" file="test_opensearchpy/test_client/test_overrides.py" line="83" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_cluster_stats" file="test_opensearchpy/test_client/test_overrides.py" line="93" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_create" file="test_opensearchpy/test_client/test_overrides.py" line="33" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_delete" file="test_opensearchpy/test_client/test_overrides.py" line="37" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_exists" file="test_opensearchpy/test_client/test_overrides.py" line="41" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_exists_source" file="test_opensearchpy/test_client/test_overrides.py" line="57" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_explain" file="test_opensearchpy/test_client/test_overrides.py" line="45" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_get" file="test_opensearchpy/test_client/test_overrides.py" line="49" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_get_source" file="test_opensearchpy/test_client/test_overrides.py" line="53" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_index" file="test_opensearchpy/test_client/test_overrides.py" line="61" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_indices_put_mapping" file="test_opensearchpy/test_client/test_overrides.py" line="100" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_mtermvectors" file="test_opensearchpy/test_client/test_overrides.py" line="75" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_tasks_get" file="test_opensearchpy/test_client/test_overrides.py" line="107" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_termvectors" file="test_opensearchpy/test_client/test_overrides.py" line="68" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_overrides.TestOverriddenUrlTargets" name="test_update" file="test_opensearchpy/test_client/test_overrides.py" line="79" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestQueryParams" name="test_handles_empty_none_and_normalization" file="test_opensearchpy/test_client/test_utils.py" line="70" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestQueryParams" name="test_handles_headers" file="test_opensearchpy/test_client/test_utils.py" line="58" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestQueryParams" name="test_handles_opaque_id" file="test_opensearchpy/test_client/test_utils.py" line="64" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestQueryParams" name="test_handles_params" file="test_opensearchpy/test_client/test_utils.py" line="43" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestQueryParams" name="test_per_call_authentication" file="test_opensearchpy/test_client/test_utils.py" line="86" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestMakePath" name="test_handles_unicode" file="test_opensearchpy/test_client/test_utils.py" line="131" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestMakePath" name="test_handles_utf_encoded_string" file="test_opensearchpy/test_client/test_utils.py" line="137" time="0.001"><skipped type="pytest.skip" message="Only relevant for py2">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_client/test_utils.py:138: Only relevant for py2</skipped></testcase><testcase classname="test_opensearchpy.test_client.test_utils.TestEscape" name="test_handles_ascii" file="test_opensearchpy/test_client/test_utils.py" line="147" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestEscape" name="test_handles_bytestring" file="test_opensearchpy/test_client/test_utils.py" line="155" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestEscape" name="test_handles_unicode" file="test_opensearchpy/test_client/test_utils.py" line="151" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestBulkBody" name="test_bulk_body_as_bytestring_adds_trailing_newline" file="test_opensearchpy/test_client/test_utils.py" line="176" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestBulkBody" name="test_bulk_body_as_string_adds_trailing_newline" file="test_opensearchpy/test_client/test_utils.py" line="169" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestBulkBody" name="test_proper_bulk_body_as_bytestring_is_not_modified" file="test_opensearchpy/test_client/test_utils.py" line="165" time="0.001" /><testcase classname="test_opensearchpy.test_client.test_utils.TestBulkBody" name="test_proper_bulk_body_as_string_is_not_modified" file="test_opensearchpy/test_client/test_utils.py" line="161" time="0.001" /><testcase classname="test_opensearchpy.test_server.test_clients.TestUnicode" name="test_indices_analyze" file="test_opensearchpy/test_server/test_clients.py" line="33" time="0.001"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_clients.TestBulk" name="test_bulk_works_with_bytestring_body" file="test_opensearchpy/test_server/test_clients.py" line="45" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_clients.TestBulk" name="test_bulk_works_with_string_body" file="test_opensearchpy/test_server/test_clients.py" line="38" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_actions_remain_unchanged" file="test_opensearchpy/test_server/test_helpers.py" line="53" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_all_documents_get_inserted" file="test_opensearchpy/test_server/test_helpers.py" line="61" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_all_errors_from_chunk_are_raised_on_failure" file="test_opensearchpy/test_server/test_helpers.py" line="73" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_different_op_types" file="test_opensearchpy/test_server/test_helpers.py" line="93" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_rejected_documents_are_retried" file="test_opensearchpy/test_server/test_helpers.py" line="151" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_rejected_documents_are_retried_at_most_max_retries_times" file="test_opensearchpy/test_server/test_helpers.py" line="178" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_transport_error_can_becaught" file="test_opensearchpy/test_server/test_helpers.py" line="115" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestStreamingBulk" name="test_transport_error_is_raised_with_max_retries" file="test_opensearchpy/test_server/test_helpers.py" line="206" time="0.001"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestBulk" name="test_all_documents_get_inserted" file="test_opensearchpy/test_server/test_helpers.py" line="243" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestBulk" name="test_bulk_works_with_single_item" file="test_opensearchpy/test_server/test_helpers.py" line="230" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestBulk" name="test_error_is_raised" file="test_opensearchpy/test_server/test_helpers.py" line="293" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestBulk" name="test_errors_are_collected_properly" file="test_opensearchpy/test_server/test_helpers.py" line="344" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestBulk" name="test_errors_are_reported_correctly" file="test_opensearchpy/test_server/test_helpers.py" line="266" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestBulk" name="test_ignore_error_if_raised" file="test_opensearchpy/test_server/test_helpers.py" line="311" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestBulk" name="test_stats_only_reports_numbers" file="test_opensearchpy/test_server/test_helpers.py" line="256" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_all_documents_are_read" file="test_opensearchpy/test_server/test_helpers.py" line="403" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_clear_scroll" file="test_opensearchpy/test_server/test_helpers.py" line="585" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_initial_search_error" file="test_opensearchpy/test_server/test_helpers.py" line="451" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_logger" file="test_opensearchpy/test_server/test_helpers.py" line="549" time="0.001"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_no_scroll_id_fast_route" file="test_opensearchpy/test_server/test_helpers.py" line="477" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_order_can_be_preserved" file="test_opensearchpy/test_server/test_helpers.py" line="383" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_scan_auth_kwargs_favor_scroll_kwargs_option" file="test_opensearchpy/test_server/test_helpers.py" line="518" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_scan_auth_kwargs_forwarded" file="test_opensearchpy/test_server/test_helpers.py" line="486" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_scroll_error" file="test_opensearchpy/test_server/test_helpers.py" line="416" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestScan" name="test_shards_no_skipped_field" file="test_opensearchpy/test_server/test_helpers.py" line="612" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestReindex" name="test_all_documents_get_moved" file="test_opensearchpy/test_server/test_helpers.py" line="692" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestReindex" name="test_reindex_accepts_a_query" file="test_opensearchpy/test_server/test_helpers.py" line="673" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestReindex" name="test_reindex_passes_kwargs_to_scan_and_bulk" file="test_opensearchpy/test_server/test_helpers.py" line="654" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_helpers.TestParentChildReindex" name="test_children_are_reindexed_correctly" file="test_opensearchpy/test_server/test_helpers.py" line="737" time="0.000"><skipped type="pytest.skip" message="No client is available">/local/home/hvamsi/clients/opensearch-py/venv/lib/python3.10/site-packages/_pytest/unittest.py:364: No client is available</skipped></testcase><testcase classname="test_opensearchpy.test_server.test_rest_api_spec" name="test_rest_api_spec[test_spec0]" file="test_opensearchpy/test_server/test_rest_api_spec.py" line="548" time="0.000"><skipped type="pytest.skip" message="got empty parameter set ['test_spec'], function test_rest_api_spec at /local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_server/test_rest_api_spec.py:548">/local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_server/test_rest_api_spec.py:549: got empty parameter set ['test_spec'], function test_rest_api_spec at /local/home/hvamsi/clients/opensearch-py/test_opensearchpy/test_server/test_rest_api_spec.py:548</skipped></testcase><testcase classname="test_opensearchpy.test_server_secured.test_clients.TestSecurity" name="test_security" file="test_opensearchpy/test_server_secured/test_clients.py" line="17" time="0.015" /></testsuite></testsuites>